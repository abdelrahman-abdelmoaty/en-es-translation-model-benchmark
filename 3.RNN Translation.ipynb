{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.12.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":692482,"sourceType":"modelInstanceVersion","isSourceIdPinned":true,"modelInstanceId":525061,"modelId":539100}],"dockerImageVersionId":31236,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import os\n#Legacy TensorFlow BackEnd\nos.environ['TF_USE_LEGACY_KERAS'] = '1'","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-12-19T15:18:21.278844Z","iopub.execute_input":"2025-12-19T15:18:21.279186Z","iopub.status.idle":"2025-12-19T15:18:21.286065Z","shell.execute_reply.started":"2025-12-19T15:18:21.279150Z","shell.execute_reply":"2025-12-19T15:18:21.285387Z"}},"outputs":[],"execution_count":1},{"cell_type":"code","source":"import numpy as np\nimport matplotlib.pyplot as plt\nimport tensorflow as tf\nfrom tensorflow.keras.preprocessing.text import Tokenizer\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\nimport pathlib\nimport tensorflow_text as tf_text\nimport pickle","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-19T15:18:21.438481Z","iopub.execute_input":"2025-12-19T15:18:21.439077Z","iopub.status.idle":"2025-12-19T15:18:37.187281Z","shell.execute_reply.started":"2025-12-19T15:18:21.439045Z","shell.execute_reply":"2025-12-19T15:18:37.186672Z"}},"outputs":[{"name":"stderr","text":"2025-12-19 15:18:23.328404: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1766157503.551194      55 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1766157503.617260      55 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\nW0000 00:00:1766157504.141439      55 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\nW0000 00:00:1766157504.141482      55 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\nW0000 00:00:1766157504.141485      55 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\nW0000 00:00:1766157504.141488      55 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"strategy = tf.distribute.MirroredStrategy()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-19T15:18:37.188420Z","iopub.execute_input":"2025-12-19T15:18:37.188946Z","iopub.status.idle":"2025-12-19T15:18:38.181013Z","shell.execute_reply.started":"2025-12-19T15:18:37.188919Z","shell.execute_reply":"2025-12-19T15:18:38.180286Z"}},"outputs":[{"name":"stdout","text":"INFO:tensorflow:Using MirroredStrategy with devices ('/job:localhost/replica:0/task:0/device:GPU:0', '/job:localhost/replica:0/task:0/device:GPU:1')\n","output_type":"stream"},{"name":"stderr","text":"I0000 00:00:1766157518.134473      55 gpu_device.cc:2019] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 13942 MB memory:  -> device: 0, name: Tesla T4, pci bus id: 0000:00:04.0, compute capability: 7.5\nI0000 00:00:1766157518.138417      55 gpu_device.cc:2019] Created device /job:localhost/replica:0/task:0/device:GPU:1 with 13942 MB memory:  -> device: 1, name: Tesla T4, pci bus id: 0000:00:05.0, compute capability: 7.5\n","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"print('Number of devices: {}'.format(strategy.num_replicas_in_sync))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-19T15:18:38.181945Z","iopub.execute_input":"2025-12-19T15:18:38.182244Z","iopub.status.idle":"2025-12-19T15:18:38.186375Z","shell.execute_reply.started":"2025-12-19T15:18:38.182219Z","shell.execute_reply":"2025-12-19T15:18:38.185541Z"}},"outputs":[{"name":"stdout","text":"Number of devices: 2\n","output_type":"stream"}],"execution_count":4},{"cell_type":"code","source":"config = {\n    \"learning_rate\": 1e-4,\n    \"batch_size\": 256,\n    \"epochs\": 20,\n    \"max_vocab_size\":5000,\n    \"max_length\":50\n}\n\nGLOBAL_BATCH = config['batch_size'] * strategy.num_replicas_in_sync\nLEARNING_RATE = config['learning_rate']\nEPOCHS = config['epochs']","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-19T15:18:38.188153Z","iopub.execute_input":"2025-12-19T15:18:38.188396Z","iopub.status.idle":"2025-12-19T15:18:38.201504Z","shell.execute_reply.started":"2025-12-19T15:18:38.188373Z","shell.execute_reply":"2025-12-19T15:18:38.200873Z"}},"outputs":[],"execution_count":5},{"cell_type":"code","source":"path_to_zip = tf.keras.utils.get_file(\n    'spa-eng.zip', origin='http://storage.googleapis.com/download.tensorflow.org/data/spa-eng.zip',\n    extract=True)\n\npath_to_file = pathlib.Path(path_to_zip).parent/'spa-eng/spa.txt'","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-19T15:18:38.202272Z","iopub.execute_input":"2025-12-19T15:18:38.202511Z","iopub.status.idle":"2025-12-19T15:18:38.363191Z","shell.execute_reply.started":"2025-12-19T15:18:38.202475Z","shell.execute_reply":"2025-12-19T15:18:38.362511Z"}},"outputs":[{"name":"stdout","text":"Downloading data from http://storage.googleapis.com/download.tensorflow.org/data/spa-eng.zip\n2638744/2638744 [==============================] - 0s 0us/step\n","output_type":"stream"}],"execution_count":6},{"cell_type":"code","source":"def load_data(path):\n  text = path.read_text(encoding='utf-8')\n\n  lines = text.splitlines()\n  pairs = [line.split('\\t') for line in lines]\n\n  context = np.array([context for target, context in pairs])\n  target = np.array([target for target, context in pairs])\n\n  return target, context","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-19T15:18:38.364136Z","iopub.execute_input":"2025-12-19T15:18:38.364423Z","iopub.status.idle":"2025-12-19T15:18:38.369801Z","shell.execute_reply.started":"2025-12-19T15:18:38.364386Z","shell.execute_reply":"2025-12-19T15:18:38.368919Z"}},"outputs":[],"execution_count":7},{"cell_type":"code","source":"context_raw,target_raw = load_data(path_to_file)\nprint(context_raw[-1])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-19T15:18:38.370754Z","iopub.execute_input":"2025-12-19T15:18:38.371038Z","iopub.status.idle":"2025-12-19T15:18:39.015651Z","shell.execute_reply.started":"2025-12-19T15:18:38.371012Z","shell.execute_reply":"2025-12-19T15:18:39.014843Z"}},"outputs":[{"name":"stdout","text":"If you want to sound like a native speaker, you must be willing to practice saying the same sentence over and over in the same way that banjo players practice the same phrase over and over until they can play it correctly and at the desired tempo.\n","output_type":"stream"}],"execution_count":8},{"cell_type":"code","source":"print(target_raw[-1])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-19T15:18:39.016697Z","iopub.execute_input":"2025-12-19T15:18:39.017253Z","iopub.status.idle":"2025-12-19T15:18:39.020577Z","shell.execute_reply.started":"2025-12-19T15:18:39.017228Z","shell.execute_reply":"2025-12-19T15:18:39.019819Z"}},"outputs":[{"name":"stdout","text":"Si quieres sonar como un hablante nativo, debes estar dispuesto a practicar diciendo la misma frase una y otra vez de la misma manera en que un músico de banjo practica el mismo fraseo una y otra vez hasta que lo puedan tocar correctamente y en el tiempo esperado.\n","output_type":"stream"}],"execution_count":9},{"cell_type":"code","source":"split_idx = int(0.9 * len(target_raw))\n\nX_train = context_raw[:split_idx]\ny_train = target_raw[:split_idx]\n\nX_val = context_raw[split_idx:]\ny_val = target_raw[split_idx:]\n\nBUFFER_SIZE = len(X_train)\n\ntrain_dataset = tf.data.Dataset.from_tensor_slices((X_train, y_train))\ntrain_dataset = train_dataset.shuffle(BUFFER_SIZE).batch(GLOBAL_BATCH, drop_remainder=True).prefetch(tf.data.AUTOTUNE)\n\n# Validation dataset (no shuffle needed)\nval_dataset = tf.data.Dataset.from_tensor_slices((X_val, y_val))\nval_dataset = val_dataset.batch(GLOBAL_BATCH, drop_remainder=True).prefetch(tf.data.AUTOTUNE)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-19T15:18:39.021667Z","iopub.execute_input":"2025-12-19T15:18:39.022010Z","iopub.status.idle":"2025-12-19T15:18:39.376755Z","shell.execute_reply.started":"2025-12-19T15:18:39.021987Z","shell.execute_reply":"2025-12-19T15:18:39.375914Z"}},"outputs":[],"execution_count":10},{"cell_type":"code","source":"TRAIN_STEPS = tf.data.experimental.cardinality(train_dataset).numpy()\nVAL_STEPS = tf.data.experimental.cardinality(val_dataset).numpy()\n\nprint(TRAIN_STEPS, VAL_STEPS)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-19T15:18:39.378748Z","iopub.execute_input":"2025-12-19T15:18:39.379236Z","iopub.status.idle":"2025-12-19T15:18:39.384526Z","shell.execute_reply.started":"2025-12-19T15:18:39.379209Z","shell.execute_reply":"2025-12-19T15:18:39.383827Z"}},"outputs":[{"name":"stdout","text":"209 23\n","output_type":"stream"}],"execution_count":11},{"cell_type":"code","source":"idx = np.random.randint(0,63)\ncontext,target = next(iter(val_dataset))\nprint(f\"Input:{context[idx].numpy().decode('utf-8')}\")\nprint(f\"Target:{target[idx].numpy().decode('utf-8')}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-19T15:18:39.385741Z","iopub.execute_input":"2025-12-19T15:18:39.386117Z","iopub.status.idle":"2025-12-19T15:18:39.441206Z","shell.execute_reply.started":"2025-12-19T15:18:39.386092Z","shell.execute_reply":"2025-12-19T15:18:39.440576Z"}},"outputs":[{"name":"stdout","text":"Input:To tell the truth, I don't want to go with them.\nTarget:La verdad, no quiero ir con ellos.\n","output_type":"stream"}],"execution_count":12},{"cell_type":"code","source":"def tf_lower_and_split_punct_w_special_tokens(text):\n  text = tf_text.normalize_utf8(text, 'NFKD')\n  text = tf.strings.lower(text)\n  text = tf.strings.regex_replace(text, '[^ a-z.?!,¿]', '')\n  text = tf.strings.regex_replace(text, '[.?!,¿]', r' \\0 ')\n  text = tf.strings.strip(text)\n\n  text = tf.strings.join(['[START]', text, '[END]'], separator=' ')\n  return text\n\ndef tf_lower_and_split_punct(text):\n  text = tf_text.normalize_utf8(text, 'NFKD')\n  text = tf.strings.lower(text)\n  text = tf.strings.regex_replace(text, '[^ a-z.?!,¿]', '')\n  text = tf.strings.regex_replace(text, '[.?!,¿]', r' \\0 ')\n  text = tf.strings.strip(text)\n\n  return text","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-19T15:18:39.442108Z","iopub.execute_input":"2025-12-19T15:18:39.442328Z","iopub.status.idle":"2025-12-19T15:18:39.447677Z","shell.execute_reply.started":"2025-12-19T15:18:39.442307Z","shell.execute_reply":"2025-12-19T15:18:39.446948Z"}},"outputs":[],"execution_count":13},{"cell_type":"code","source":"print(target[idx].numpy().decode())\nprint(tf_lower_and_split_punct(target[idx]).numpy().decode())\nprint(tf_lower_and_split_punct_w_special_tokens(target[idx]).numpy().decode())","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-19T15:18:39.448687Z","iopub.execute_input":"2025-12-19T15:18:39.448998Z","iopub.status.idle":"2025-12-19T15:18:39.475369Z","shell.execute_reply.started":"2025-12-19T15:18:39.448969Z","shell.execute_reply":"2025-12-19T15:18:39.474644Z"}},"outputs":[{"name":"stdout","text":"La verdad, no quiero ir con ellos.\nla verdad ,  no quiero ir con ellos .\n[START] la verdad ,  no quiero ir con ellos . [END]\n","output_type":"stream"}],"execution_count":14},{"cell_type":"code","source":"with open('/kaggle/input/vocab/tensorflow2/default/1/context_vocab.pkl', 'rb') as f:\n    context_vocab = pickle.load(f)\n\nwith open('/kaggle/input/vocab/tensorflow2/default/1/target_vocab.pkl', 'rb') as f:\n    target_vocab = pickle.load(f)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-19T15:19:36.791768Z","iopub.execute_input":"2025-12-19T15:19:36.792558Z","iopub.status.idle":"2025-12-19T15:19:36.824416Z","shell.execute_reply.started":"2025-12-19T15:19:36.792522Z","shell.execute_reply":"2025-12-19T15:19:36.823657Z"}},"outputs":[],"execution_count":15},{"cell_type":"code","source":"context_text_processor = tf.keras.layers.TextVectorization(\n    standardize = tf_lower_and_split_punct,\n    max_tokens = config['max_vocab_size'],\n    output_sequence_length = config['max_length'],\n    vocabulary = context_vocab,\n    ragged = False)\n\ntarget_text_processor = tf.keras.layers.TextVectorization(\n    standardize = tf_lower_and_split_punct_w_special_tokens,\n    max_tokens = config['max_vocab_size'],\n    output_sequence_length = config['max_length'] + 1,\n    vocabulary = target_vocab,\n    ragged = False)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-19T15:20:28.241319Z","iopub.execute_input":"2025-12-19T15:20:28.241625Z","iopub.status.idle":"2025-12-19T15:20:28.327234Z","shell.execute_reply.started":"2025-12-19T15:20:28.241598Z","shell.execute_reply":"2025-12-19T15:20:28.326614Z"}},"outputs":[],"execution_count":17},{"cell_type":"code","source":"# context_text_processor.adapt(train_dataset.map(lambda context, target: context))\n\nprint(context_text_processor.get_vocabulary()[:10])\n\n\n# target_text_processor.adapt(train_dataset.map(lambda context, target: target))\ntarget_text_processor.get_vocabulary()[:10]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-19T15:20:36.268245Z","iopub.execute_input":"2025-12-19T15:20:36.268838Z","iopub.status.idle":"2025-12-19T15:20:36.298502Z","shell.execute_reply.started":"2025-12-19T15:20:36.268809Z","shell.execute_reply":"2025-12-19T15:20:36.297896Z"}},"outputs":[{"name":"stdout","text":"['', '[UNK]', np.str_('.'), np.str_('i'), np.str_('the'), np.str_('to'), np.str_('you'), np.str_('tom'), np.str_('?'), np.str_('a')]\n","output_type":"stream"},{"execution_count":18,"output_type":"execute_result","data":{"text/plain":"['',\n '[UNK]',\n np.str_('[START]'),\n np.str_('[END]'),\n np.str_('.'),\n np.str_('que'),\n np.str_('el'),\n np.str_('de'),\n np.str_('no'),\n np.str_('tom')]"},"metadata":{}}],"execution_count":18},{"cell_type":"code","source":"context_vocab = context_text_processor.get_vocabulary()\nwith open('context_vocab.pkl', 'wb') as f:\n    pickle.dump(context_vocab, f)\n\ntarget_vocab = target_text_processor.get_vocabulary()\n\nwith open('target_vocab.pkl', 'wb') as f:\n    pickle.dump(target_vocab, f)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-19T14:04:38.367491Z","iopub.execute_input":"2025-12-19T14:04:38.367877Z","iopub.status.idle":"2025-12-19T14:04:38.430731Z","shell.execute_reply.started":"2025-12-19T14:04:38.367848Z","shell.execute_reply":"2025-12-19T14:04:38.430198Z"}},"outputs":[],"execution_count":17},{"cell_type":"code","source":"def process_text(context, target):\n  context = context_text_processor(context)\n  target = target_text_processor(target)\n  targ_in = target[:,:-1]\n  targ_out = target[:,1:]\n  return (context, targ_in), targ_out\n\n\ntrain_dataset = train_dataset.map(process_text, num_parallel_calls=tf.data.AUTOTUNE)\nval_dataset = val_dataset.map(process_text, num_parallel_calls = tf.data.AUTOTUNE)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-19T15:20:48.966585Z","iopub.execute_input":"2025-12-19T15:20:48.967583Z","iopub.status.idle":"2025-12-19T15:20:49.145970Z","shell.execute_reply.started":"2025-12-19T15:20:48.967530Z","shell.execute_reply":"2025-12-19T15:20:49.145186Z"}},"outputs":[],"execution_count":19},{"cell_type":"code","source":"batch = next(iter(val_dataset))\nreplica_batch = strategy.experimental_local_results(batch)[0]\n(ex_context_tok, ex_tar_in), ex_tar_out = replica_batch\nprint(ex_context_tok[0, :10].numpy()) \nprint(ex_context_tok.shape)\nprint(ex_tar_in[0, :10].numpy()) \nprint(ex_tar_out[0, :10].numpy())\nprint(ex_tar_out.shape)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-19T15:20:49.167598Z","iopub.execute_input":"2025-12-19T15:20:49.168211Z","iopub.status.idle":"2025-12-19T15:20:49.257259Z","shell.execute_reply.started":"2025-12-19T15:20:49.168174Z","shell.execute_reply":"2025-12-19T15:20:49.256152Z"}},"outputs":[{"name":"stdout","text":"[  18   10    4  297 4408   19    3  212  179    2]\n(512, 50)\n[   2   42 4238   14 1118   36    5  592  137    4]\n[  42 4238   14 1118   36    5  592  137    4    3]\n(512, 50)\n","output_type":"stream"}],"execution_count":20},{"cell_type":"code","source":"def Build_Seq2Seq(\n    max_length=157,\n    vocab_size_en=10000,\n    vocab_size_es=10000,\n    embedding_dim=256,\n    units=256\n):\n    # ================= Encoder =================\n    encoder_input = tf.keras.layers.Input(\n        shape=(max_length,), dtype=\"int32\", name=\"encoder_input\"\n    )\n\n    enc_emb = tf.keras.layers.Embedding(\n        vocab_size_en, embedding_dim, mask_zero=True\n    )(encoder_input)\n    enc_emb = tf.keras.layers.Dropout(0.2)(enc_emb)\n\n    encoder = tf.keras.layers.Bidirectional(\n        tf.keras.layers.SimpleRNN(\n            units,\n            return_sequences=True,\n            return_state=True,\n            name=\"encoder_rnn\"\n        )\n    )\n\n    encoder_outputs, forward_h, backward_h = encoder(enc_emb)\n\n    # Concatenate forward + backward hidden states\n    encoder_state_h = tf.keras.layers.Concatenate(axis=-1)(\n        [forward_h, backward_h]\n    )  # shape = (batch, units*2)\n\n    # ================= Decoder =================\n    decoder_input = tf.keras.layers.Input(\n        shape=(max_length,), dtype=\"int32\", name=\"decoder_input\"\n    )\n\n    dec_emb = tf.keras.layers.Embedding(\n        vocab_size_es, embedding_dim, mask_zero=True\n    )(decoder_input)\n    dec_emb = tf.keras.layers.Dropout(0.2)(dec_emb)\n\n    decoder_outputs = tf.keras.layers.SimpleRNN(\n        units * 2,                 # MUST match encoder_state_h\n        return_sequences=True,\n        name=\"decoder_rnn\"\n    )(dec_emb, initial_state=[encoder_state_h])\n\n    # ================= Cross Attention =================\n    attention_output = tf.keras.layers.MultiHeadAttention(\n        num_heads=8,\n        key_dim=units * 2,\n        name=\"cross_attention\"\n    )(\n        query=decoder_outputs,\n        value=encoder_outputs,\n        key=encoder_outputs\n    )\n\n    # ================= Output =================\n    decoder_combined = tf.keras.layers.Concatenate(axis=-1)(\n        [decoder_outputs, attention_output]\n    )\n\n    outputs = tf.keras.layers.Dense(\n        vocab_size_es, activation=\"softmax\", name=\"output_dense\"\n    )(decoder_combined)\n\n    model = tf.keras.Model(\n        inputs=[encoder_input, decoder_input],\n        outputs=outputs,\n        name=\"seq2seq_rnn\"\n    )\n\n    return model\n\nvocab_size_en = context_text_processor.vocabulary_size()\nvocab_size_fr = target_text_processor.vocabulary_size()\nprint(f\"English vocab size: {vocab_size_en}\")\nprint(f\"French vocab size: {vocab_size_fr}\")\n\nwith strategy.scope():\n    model = Build_Seq2Seq(\n        max_length=config['max_length'], \n        vocab_size_en=vocab_size_en,\n        vocab_size_es=vocab_size_fr,\n        embedding_dim=256,\n        units=256  \n      \n    )","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-19T15:20:50.987660Z","iopub.execute_input":"2025-12-19T15:20:50.988052Z","iopub.status.idle":"2025-12-19T15:20:52.244573Z","shell.execute_reply.started":"2025-12-19T15:20:50.988023Z","shell.execute_reply":"2025-12-19T15:20:52.243850Z"}},"outputs":[{"name":"stdout","text":"English vocab size: 5000\nFrench vocab size: 5000\n","output_type":"stream"}],"execution_count":21},{"cell_type":"code","source":"model.summary()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-19T15:20:53.804952Z","iopub.execute_input":"2025-12-19T15:20:53.805255Z","iopub.status.idle":"2025-12-19T15:20:53.837825Z","shell.execute_reply.started":"2025-12-19T15:20:53.805229Z","shell.execute_reply":"2025-12-19T15:20:53.837252Z"}},"outputs":[{"name":"stdout","text":"Model: \"seq2seq_rnn\"\n__________________________________________________________________________________________________\n Layer (type)                Output Shape                 Param #   Connected to                  \n==================================================================================================\n encoder_input (InputLayer)  [(None, 50)]                 0         []                            \n                                                                                                  \n embedding (Embedding)       (None, 50, 256)              1280000   ['encoder_input[0][0]']       \n                                                                                                  \n decoder_input (InputLayer)  [(None, 50)]                 0         []                            \n                                                                                                  \n dropout (Dropout)           (None, 50, 256)              0         ['embedding[0][0]']           \n                                                                                                  \n embedding_1 (Embedding)     (None, 50, 256)              1280000   ['decoder_input[0][0]']       \n                                                                                                  \n bidirectional (Bidirection  [(None, 50, 512),            262656    ['dropout[0][0]']             \n al)                          (None, 256),                                                        \n                              (None, 256)]                                                        \n                                                                                                  \n dropout_1 (Dropout)         (None, 50, 256)              0         ['embedding_1[0][0]']         \n                                                                                                  \n concatenate (Concatenate)   (None, 512)                  0         ['bidirectional[0][1]',       \n                                                                     'bidirectional[0][2]']       \n                                                                                                  \n decoder_rnn (SimpleRNN)     (None, 50, 512)              393728    ['dropout_1[0][0]',           \n                                                                     'concatenate[0][0]']         \n                                                                                                  \n cross_attention (MultiHead  (None, 50, 512)              8401408   ['decoder_rnn[0][0]',         \n Attention)                                                          'bidirectional[0][0]',       \n                                                                     'bidirectional[0][0]']       \n                                                                                                  \n concatenate_1 (Concatenate  (None, 50, 1024)             0         ['decoder_rnn[0][0]',         \n )                                                                   'cross_attention[0][0]']     \n                                                                                                  \n output_dense (Dense)        (None, 50, 5000)             5125000   ['concatenate_1[0][0]']       \n                                                                                                  \n==================================================================================================\nTotal params: 16742792 (63.87 MB)\nTrainable params: 16742792 (63.87 MB)\nNon-trainable params: 0 (0.00 Byte)\n__________________________________________________________________________________________________\n","output_type":"stream"}],"execution_count":22},{"cell_type":"code","source":"def masked_loss(y_true, y_pred):\n    \n    loss_fn = tf.keras.losses.SparseCategoricalCrossentropy(\n        from_logits=False,\n        reduction='none'\n    )\n    loss = loss_fn(y_true, y_pred) \n\n    mask = tf.cast(y_true != 0, loss.dtype)\n    loss *= mask\n\n    return tf.reduce_sum(loss) / tf.reduce_sum(mask)\n\n\ndef masked_acc(y_true, y_pred):\n    \n    y_pred = tf.argmax(y_pred, axis=-1)\n    y_pred = tf.cast(y_pred, y_true.dtype)\n\n    match = tf.cast(y_true == y_pred, tf.float32)\n    \n    mask = tf.cast(y_true != 0, tf.float32)\n\n    return tf.reduce_sum(match) / tf.reduce_sum(mask)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-19T15:20:57.296064Z","iopub.execute_input":"2025-12-19T15:20:57.296667Z","iopub.status.idle":"2025-12-19T15:20:57.302057Z","shell.execute_reply.started":"2025-12-19T15:20:57.296639Z","shell.execute_reply":"2025-12-19T15:20:57.301293Z"}},"outputs":[],"execution_count":23},{"cell_type":"code","source":"with strategy.scope():\n    model.compile(optimizer='adam',\n                  loss=masked_loss, \n                  metrics=[masked_acc])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-19T15:20:57.751110Z","iopub.execute_input":"2025-12-19T15:20:57.751431Z","iopub.status.idle":"2025-12-19T15:20:57.782475Z","shell.execute_reply.started":"2025-12-19T15:20:57.751404Z","shell.execute_reply":"2025-12-19T15:20:57.781934Z"}},"outputs":[],"execution_count":24},{"cell_type":"code","source":"history = model.fit(\n    train_dataset, \n    epochs=30,\n    validation_data=val_dataset,\n    callbacks=[\n        tf.keras.callbacks.EarlyStopping(\n            monitor='val_loss',\n            patience=5,\n            restore_best_weights=True\n        ),\n        tf.keras.callbacks.ReduceLROnPlateau(\n            monitor='val_loss',\n            factor=0.1,          \n            patience=3,           \n            min_lr=1e-7,          \n            verbose=1             \n        )\n    ]\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-19T15:20:59.693164Z","iopub.execute_input":"2025-12-19T15:20:59.693487Z","iopub.status.idle":"2025-12-19T15:52:03.252330Z","shell.execute_reply.started":"2025-12-19T15:20:59.693461Z","shell.execute_reply":"2025-12-19T15:52:03.251413Z"}},"outputs":[{"name":"stdout","text":"Epoch 1/30\nINFO:tensorflow:Collective all_reduce tensors: 19 all_reduces, num_devices = 2, group_size = 2, implementation = CommunicationImplementation.NCCL, num_packs = 1\nINFO:tensorflow:Collective all_reduce IndexedSlices: 2 all_reduces, num_devices =2, group_size = 2, implementation = CommunicationImplementation.NCCL\nINFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\nINFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\nINFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\nINFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\nINFO:tensorflow:Collective all_reduce tensors: 19 all_reduces, num_devices = 2, group_size = 2, implementation = CommunicationImplementation.NCCL, num_packs = 1\nINFO:tensorflow:Collective all_reduce IndexedSlices: 2 all_reduces, num_devices =2, group_size = 2, implementation = CommunicationImplementation.NCCL\nINFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\nINFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\nINFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\nINFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n","output_type":"stream"},{"name":"stderr","text":"WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nI0000 00:00:1766157672.429856     124 service.cc:152] XLA service 0x7a5a68da1670 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\nI0000 00:00:1766157672.429923     124 service.cc:160]   StreamExecutor device (0): Tesla T4, Compute Capability 7.5\nI0000 00:00:1766157672.429928     124 service.cc:160]   StreamExecutor device (1): Tesla T4, Compute Capability 7.5\nI0000 00:00:1766157672.550254     124 cuda_dnn.cc:529] Loaded cuDNN version 91002\nI0000 00:00:1766157672.550272     122 cuda_dnn.cc:529] Loaded cuDNN version 91002\nI0000 00:00:1766157672.894016     122 device_compiler.h:188] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n","output_type":"stream"},{"name":"stdout","text":"209/209 [==============================] - ETA: 0s - loss: 3.7806 - masked_acc: 0.3923INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\nINFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n209/209 [==============================] - 161s 698ms/step - loss: 3.7806 - masked_acc: 0.3923 - val_loss: 3.4482 - val_masked_acc: 0.4004 - lr: 0.0010\nEpoch 2/30\n209/209 [==============================] - 126s 602ms/step - loss: 1.7673 - masked_acc: 0.6542 - val_loss: 2.7129 - val_masked_acc: 0.4905 - lr: 0.0010\nEpoch 3/30\n209/209 [==============================] - 121s 579ms/step - loss: 1.2410 - masked_acc: 0.7304 - val_loss: 2.3104 - val_masked_acc: 0.5422 - lr: 0.0010\nEpoch 4/30\n209/209 [==============================] - 123s 587ms/step - loss: 1.0231 - masked_acc: 0.7629 - val_loss: 2.1967 - val_masked_acc: 0.5630 - lr: 0.0010\nEpoch 5/30\n209/209 [==============================] - 121s 579ms/step - loss: 0.8911 - masked_acc: 0.7850 - val_loss: 2.2065 - val_masked_acc: 0.5621 - lr: 0.0010\nEpoch 6/30\n209/209 [==============================] - 122s 582ms/step - loss: 0.7950 - masked_acc: 0.8017 - val_loss: 2.1684 - val_masked_acc: 0.5717 - lr: 0.0010\nEpoch 7/30\n209/209 [==============================] - 121s 581ms/step - loss: 0.7177 - masked_acc: 0.8149 - val_loss: 2.2153 - val_masked_acc: 0.5681 - lr: 0.0010\nEpoch 8/30\n209/209 [==============================] - 120s 576ms/step - loss: 0.6570 - masked_acc: 0.8267 - val_loss: 2.1911 - val_masked_acc: 0.5725 - lr: 0.0010\nEpoch 9/30\n209/209 [==============================] - ETA: 0s - loss: 0.6044 - masked_acc: 0.8370\nEpoch 9: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\n209/209 [==============================] - 123s 586ms/step - loss: 0.6044 - masked_acc: 0.8370 - val_loss: 2.3197 - val_masked_acc: 0.5638 - lr: 0.0010\nEpoch 10/30\n209/209 [==============================] - 121s 577ms/step - loss: 0.4455 - masked_acc: 0.8764 - val_loss: 2.1643 - val_masked_acc: 0.5898 - lr: 1.0000e-04\nEpoch 11/30\n209/209 [==============================] - 120s 572ms/step - loss: 0.4019 - masked_acc: 0.8873 - val_loss: 2.1940 - val_masked_acc: 0.5895 - lr: 1.0000e-04\nEpoch 12/30\n209/209 [==============================] - 120s 576ms/step - loss: 0.3829 - masked_acc: 0.8920 - val_loss: 2.2218 - val_masked_acc: 0.5889 - lr: 1.0000e-04\nEpoch 13/30\n209/209 [==============================] - ETA: 0s - loss: 0.3692 - masked_acc: 0.8953\nEpoch 13: ReduceLROnPlateau reducing learning rate to 1.0000000656873453e-05.\n209/209 [==============================] - 120s 575ms/step - loss: 0.3692 - masked_acc: 0.8953 - val_loss: 2.2527 - val_masked_acc: 0.5877 - lr: 1.0000e-04\nEpoch 14/30\n209/209 [==============================] - 120s 575ms/step - loss: 0.3469 - masked_acc: 0.9013 - val_loss: 2.2484 - val_masked_acc: 0.5884 - lr: 1.0000e-05\nEpoch 15/30\n209/209 [==============================] - 121s 576ms/step - loss: 0.3444 - masked_acc: 0.9021 - val_loss: 2.2460 - val_masked_acc: 0.5887 - lr: 1.0000e-05\n","output_type":"stream"}],"execution_count":25},{"cell_type":"code","source":"model.save(\"BI-RNN-Cross-ATT.keras\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-19T15:54:19.238056Z","iopub.execute_input":"2025-12-19T15:54:19.238634Z","iopub.status.idle":"2025-12-19T15:54:19.943499Z","shell.execute_reply.started":"2025-12-19T15:54:19.238604Z","shell.execute_reply":"2025-12-19T15:54:19.942707Z"}},"outputs":[],"execution_count":26},{"cell_type":"code","source":"model.save_weights(\"BI-RNN-Cross-ATT.h5\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-19T15:54:19.945087Z","iopub.execute_input":"2025-12-19T15:54:19.945601Z","iopub.status.idle":"2025-12-19T15:54:20.044618Z","shell.execute_reply.started":"2025-12-19T15:54:19.945578Z","shell.execute_reply":"2025-12-19T15:54:20.043958Z"}},"outputs":[],"execution_count":27},{"cell_type":"code","source":"loaded_model = tf.keras.models.load_model(\"/kaggle/working/BI-RNN-Cross-ATT.keras\",compile=False)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-19T15:54:21.751782Z","iopub.execute_input":"2025-12-19T15:54:21.752410Z","iopub.status.idle":"2025-12-19T15:54:22.265119Z","shell.execute_reply.started":"2025-12-19T15:54:21.752381Z","shell.execute_reply":"2025-12-19T15:54:22.264456Z"}},"outputs":[],"execution_count":28},{"cell_type":"code","source":"def get_initial_state(model, context, target_text_processor):\n    if len(context.shape) == 1:\n        context = tf.expand_dims(context, 0)\n    \n    batch_size = tf.shape(context)[0]\n    vocab = target_text_processor.get_vocabulary()\n    start_token = vocab.index('[START]')\n    \n    next_token = tf.fill([batch_size, 1], start_token)\n    done = tf.zeros([batch_size, 1], dtype=tf.bool)\n    \n    return next_token, done, context\n\n\ndef get_next_token(model, context, next_token, done, state, target_text_processor, temperature=0.0):\n    vocab = target_text_processor.get_vocabulary()\n    end_token = vocab.index('[END]')\n    \n    padded_state = tf.pad(state, [[0, 0], [0, config['max_length'] - tf.shape(state)[1]]])[:, :config['max_length']]\n    logits = model.predict([context, padded_state], verbose=0)\n    last_logits = logits[:, tf.shape(state)[1] - 1, :]\n    \n    if temperature == 0.0:\n        next_token_id = tf.argmax(last_logits, axis=-1, output_type=tf.int32)\n    else:\n        next_token_id = tf.squeeze(tf.random.categorical(last_logits / temperature, 1, dtype=tf.int32), -1)\n    \n    next_token_id = tf.expand_dims(next_token_id, -1)\n    done = done | (next_token_id == end_token)\n    next_token_id = tf.where(done, tf.constant(0, dtype=tf.int32), next_token_id)\n    new_state = tf.concat([state, next_token_id], axis=1)\n    \n    return next_token_id, done, new_state\n\n\ndef translate(model, spanish_text, target_text_processor, temperature=0.0):\n    if len(spanish_text.shape) == 1:\n        spanish_text = tf.expand_dims(spanish_text, 0)\n    \n    next_token, done, context = get_initial_state(model, spanish_text, target_text_processor)\n    state = next_token\n    tokens = []\n    \n    for n in range(config['max_length']):\n        next_token, done, state = get_next_token(model, context, next_token, done, state, target_text_processor, temperature)\n        tokens.append(next_token)\n        if tf.reduce_all(done):\n            break\n    \n    tokens = tf.concat(tokens, axis=-1)\n    vocab = target_text_processor.get_vocabulary()\n    \n    words = []\n    for token_id in tokens[0].numpy():\n        if token_id == 0:\n            break\n        word = vocab[token_id]\n        if word == '[END]':\n            break\n        if word not in ['[START]', '[UNK]', '']:\n            words.append(word)\n    \n    return ' '.join(words)\n\n\ndef compare_translations(model, spanish_input, target_out, context_text_processor, target_text_processor, n=5):\n    spanish_vocab = context_text_processor.get_vocabulary()\n    english_vocab = target_text_processor.get_vocabulary()\n    \n    for i in range(min(n, spanish_input.shape[0])):\n        # Spanish input\n        sp_words = [spanish_vocab[t] for t in spanish_input[i].numpy() if t > 0]\n        spanish = ' '.join(sp_words)\n        \n        # Ground truth English\n        gt_words = [english_vocab[t] for t in target_out[i].numpy() \n                    if t > 0 and english_vocab[t] not in ['[START]', '[END]']]\n        ground_truth = ' '.join(gt_words)\n        \n        # Model translation\n        model_output = translate(model, spanish_input[i], target_text_processor)\n        \n        print(f\"\\n{i+1}. English: {spanish}\")\n        print(f\"   GROUNDTRUTH: {ground_truth}\")\n        print(f\"   TRANSLATION: {model_output}\")\n\n\n# Create distributed dataset + iterator\ndist_val_dataset = strategy.experimental_distribute_dataset(val_dataset)\ndist_val_iter = iter(dist_val_dataset)\n\n# ===== FIRST BATCH =====\n(ex_context_tok, ex_tar_in), ex_tar_out = next(dist_val_iter)\nex_context_tok = strategy.experimental_local_results(ex_context_tok)[0]\nex_tar_out     = strategy.experimental_local_results(ex_tar_out)[0]\n\ncompare_translations(\n    loaded_model,\n    ex_context_tok,\n    ex_tar_out,\n    context_text_processor,\n    target_text_processor,\n    n=5\n)\n\n# ===== SECOND BATCH =====\n(ex_context_tok, ex_tar_in), ex_tar_out = next(dist_val_iter)\nex_context_tok = strategy.experimental_local_results(ex_context_tok)[0]\nex_tar_out     = strategy.experimental_local_results(ex_tar_out)[0]\n\ncompare_translations(\n    loaded_model,\n    ex_context_tok,\n    ex_tar_out,\n    context_text_processor,\n    target_text_processor,\n    n=5\n)\n\n# ===== THIRD BATCH =====\n(ex_context_tok, ex_tar_in), ex_tar_out = next(dist_val_iter)\nex_context_tok = strategy.experimental_local_results(ex_context_tok)[0]\nex_tar_out     = strategy.experimental_local_results(ex_tar_out)[0]\n\ncompare_translations(\n    loaded_model,\n    ex_context_tok,\n    ex_tar_out,\n    context_text_processor,\n    target_text_processor,\n    n=5\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-19T15:54:55.123581Z","iopub.execute_input":"2025-12-19T15:54:55.123960Z","iopub.status.idle":"2025-12-19T15:55:12.421570Z","shell.execute_reply.started":"2025-12-19T15:54:55.123928Z","shell.execute_reply":"2025-12-19T15:55:12.420916Z"}},"outputs":[{"name":"stdout","text":"\n1. English: this is the same necklace that i lost yesterday .\n   GROUNDTRUTH: este collar es igual al que perdi ayer .\n   TRANSLATION: este es el mismo collar que perdi ayer .\n\n2. English: this is the strongest dog that i have ever seen .\n   GROUNDTRUTH: este es el perro mas fuerte que jamas haya visto .\n   TRANSLATION: este es el perro mas linda que haya visto jamas .\n\n3. English: this is your last chance to spend time with tom .\n   GROUNDTRUTH: esta es tu ultima oportunidad de pasar tiempo con tom .\n   TRANSLATION: esta es tu ultima oportunidad para pasar el tiempo con tom .\n\n4. English: this material will stand up to lots of [UNK] .\n   GROUNDTRUTH: este material [UNK] un monton de [UNK] .\n   TRANSLATION: este se a un monton de .\n\n5. English: this medicine should be taken every three hours .\n   GROUNDTRUTH: este medicamento debe ser tomado cada tres horas .\n   TRANSLATION: esta medicina deberia estar cada tres horas .\n\n1. English: his salary is double what it was seven years ago .\n   GROUNDTRUTH: su sueldo es el doble del de hace siete anos atras .\n   TRANSLATION: su salario es doble lo que hace siete anos .\n\n2. English: how come you know so much about japanese history ?\n   GROUNDTRUTH: ¿ como es que sabes tanto sobre la historia de japon ?\n   TRANSLATION: ¿ como te tanto acerca de japon acerca de japon ?\n\n3. English: how many christmas cards did you write last year ?\n   GROUNDTRUTH: ¿ cuantos [UNK] [UNK] el ano pasado ?\n   TRANSLATION: ¿ cuantas tarjetas de navidad pusiste el ano pasado ?\n\n4. English: how many books do you think you have read so far ?\n   GROUNDTRUTH: ¿ cuantos libros crees que has leido hasta ahora ?\n   TRANSLATION: ¿ cuantos libros crees que tienes hasta hasta ahora ?\n\n5. English: how many hours a day do you spend in your office ?\n   GROUNDTRUTH: ¿ cuantas horas al dia pasas en tu oficina ?\n   TRANSLATION: ¿ cuantas horas hay en menos tu oficina ?\n\n1. English: the soldiers were ready to die for their country .\n   GROUNDTRUTH: los soldados estaban [UNK] a morir por su pais .\n   TRANSLATION: los soldados estaban listos para morir por su pais .\n\n2. English: the storm prevented me from going out for a walk .\n   GROUNDTRUTH: la tormenta me impidio el salir a dar un paseo .\n   TRANSLATION: la tormenta me impidio salir a caminar .\n\n3. English: the [UNK] becomes [UNK] as you move [UNK] .\n   GROUNDTRUTH: la [UNK] es menos profundo a medida que [UNK] por el .\n   TRANSLATION: el se vuelve a como .\n\n4. English: the strong wind [UNK] that a storm is coming .\n   GROUNDTRUTH: el fuerte viento [UNK] que se acerca una tormenta .\n   TRANSLATION: el viento fuerte que se ha la tormenta .\n\n5. English: the students are happy , but the teachers are not .\n   GROUNDTRUTH: los estudiantes son felices , pero los profesores no lo son .\n   TRANSLATION: los estudiantes estan felices , pero el profesores son .\n","output_type":"stream"}],"execution_count":30},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}