{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "368d3a93",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "#Legacy TensorFlow BackEnd\n",
    "os.environ['TF_USE_LEGACY_KERAS'] = '1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ae19f305",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import pathlib\n",
    "import unicodedata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d5794931",
   "metadata": {},
   "outputs": [],
   "source": [
    "config={\n",
    "    \"max_vocab_size\":5000,\n",
    "    \"max_length\":50,\n",
    "    \"BATCH_SIZE\":64,\n",
    "    \"Split_Ratio\":0.9,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "47d71bf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "path_to_zip = tf.keras.utils.get_file(\n",
    "    'spa-eng.zip', origin='http://storage.googleapis.com/download.tensorflow.org/data/spa-eng.zip',\n",
    "    extract=True)\n",
    "\n",
    "path_to_file = pathlib.Path(path_to_zip).parent/'spa-eng/spa.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "56a21320",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(path):\n",
    "  text = path.read_text(encoding='utf-8')\n",
    "\n",
    "  lines = text.splitlines()\n",
    "  pairs = [line.split('\\t') for line in lines]\n",
    "\n",
    "  context = np.array([context for target, context in pairs])\n",
    "  target = np.array([target for target, context in pairs])\n",
    "\n",
    "  return target, context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "9cfb3a73",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "If you want to sound like a native speaker, you must be willing to practice saying the same sentence over and over in the same way that banjo players practice the same phrase over and over until they can play it correctly and at the desired tempo.\n"
     ]
    }
   ],
   "source": [
    "context_raw,target_raw = load_data(path_to_file)\n",
    "print(context_raw[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "89771625",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Si quieres sonar como un hablante nativo, debes estar dispuesto a practicar diciendo la misma frase una y otra vez de la misma manera en que un músico de banjo practica el mismo fraseo una y otra vez hasta que lo puedan tocar correctamente y en el tiempo esperado.\n"
     ]
    }
   ],
   "source": [
    "print(target_raw[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "83ce9f27",
   "metadata": {},
   "outputs": [],
   "source": [
    "split_idx = int(config[\"Split_Ratio\"] * len(target_raw))\n",
    "\n",
    "X_train = context_raw[:split_idx]\n",
    "y_train = target_raw[:split_idx]\n",
    "\n",
    "X_val = context_raw[split_idx:]\n",
    "y_val = target_raw[split_idx:]\n",
    "\n",
    "BUFFER_SIZE = len(X_train)\n",
    "\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices((X_train, y_train))\n",
    "train_dataset = train_dataset.shuffle(BUFFER_SIZE).batch(config['BATCH_SIZE'], drop_remainder=True).prefetch(tf.data.AUTOTUNE)\n",
    "\n",
    "# Validation dataset (no shuffle needed)\n",
    "val_dataset = tf.data.Dataset.from_tensor_slices((X_val, y_val))\n",
    "val_dataset = val_dataset.batch(config['BATCH_SIZE'], drop_remainder=True).prefetch(tf.data.AUTOTUNE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "b4fccf97",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input:Tom didn't come home last night. I hope he's OK.\n",
      "Target:Anoche Tom no volvió a casa, espero que esté bien.\n"
     ]
    }
   ],
   "source": [
    "idx = np.random.randint(0,63)\n",
    "context,target = next(iter(val_dataset))\n",
    "print(f\"Input:{context[idx].numpy().decode('utf-8')}\")\n",
    "print(f\"Target:{target[idx].numpy().decode('utf-8')}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "0482b74a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tf_lower_and_split_punct_w_special_tokens(text):\n",
    "  text = tf.strings.lower(text)\n",
    "  text = tf.strings.regex_replace(text, '[^ a-z.?!,¿]', '')\n",
    "  text = tf.strings.regex_replace(text, '[.?!,¿]', r' \\0 ')\n",
    "  text = tf.strings.strip(text)\n",
    "\n",
    "  text = tf.strings.join(['[START]', text, '[END]'], separator=' ')\n",
    "  return text\n",
    "\n",
    "def tf_lower_and_split_punct(text):\n",
    "  text = tf.strings.lower(text)\n",
    "  text = tf.strings.regex_replace(text, '[^ a-z.?!,¿]', '')\n",
    "  text = tf.strings.regex_replace(text, '[.?!,¿]', r' \\0 ')\n",
    "  text = tf.strings.strip(text)\n",
    "\n",
    "  return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "71a377ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Anoche Tom no volvió a casa, espero que esté bien.\n",
      "anoche tom no volvi a casa ,  espero que est bien .\n",
      "[START] anoche tom no volvi a casa ,  espero que est bien . [END]\n"
     ]
    }
   ],
   "source": [
    "print(target[idx].numpy().decode())\n",
    "print(tf_lower_and_split_punct(target[idx]).numpy().decode())\n",
    "print(tf_lower_and_split_punct_w_special_tokens(target[idx]).numpy().decode())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "119f3238",
   "metadata": {},
   "outputs": [],
   "source": [
    "context_text_processor = tf.keras.layers.TextVectorization(\n",
    "    standardize=tf_lower_and_split_punct,\n",
    "    max_tokens=config['max_vocab_size'],\n",
    "    output_sequence_length = config['max_length'],\n",
    "    ragged=False)\n",
    "\n",
    "target_text_processor = tf.keras.layers.TextVectorization(\n",
    "    standardize=tf_lower_and_split_punct_w_special_tokens,\n",
    "    max_tokens=config['max_vocab_size'],\n",
    "    output_sequence_length = config['max_length'] + 1,\n",
    "    ragged=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "4267b25a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\yassi\\anaconda3\\Lib\\site-packages\\tf_keras\\src\\utils\\tf_utils.py:492: The name tf.ragged.RaggedTensorValue is deprecated. Please use tf.compat.v1.ragged.RaggedTensorValue instead.\n",
      "\n",
      "['', '[UNK]', '.', 'i', 'the', 'to', 'you', 'tom', '?', 'a']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['', '[UNK]', '[START]', '[END]', '.', 'de', 'no', 'tom', 'a', 'que']"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "context_text_processor.adapt(train_dataset.map(lambda context, target: context))\n",
    "\n",
    "print(context_text_processor.get_vocabulary()[:10])\n",
    "\n",
    "\n",
    "target_text_processor.adapt(train_dataset.map(lambda context, target: target))\n",
    "target_text_processor.get_vocabulary()[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "e710aac0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_text(context, target):\n",
    "  context = context_text_processor(context)\n",
    "  target = target_text_processor(target)\n",
    "  targ_in = target[:,:-1]\n",
    "  targ_out = target[:,1:]\n",
    "  return (context, targ_in), targ_out\n",
    "\n",
    "\n",
    "train_dataset = train_dataset.map(process_text, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "val_dataset = val_dataset.map(process_text, num_parallel_calls = tf.data.AUTOTUNE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "51f421a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[  18   10    4  297 4406   19    3  212  179    2]\n",
      "(64, 50)\n",
      "[   2   46 4227   14 1118   37    9  602  145    4]\n",
      "[  46 4227   14 1118   37    9  602  145    4    3]\n",
      "(64, 50)\n"
     ]
    }
   ],
   "source": [
    "for (ex_context_tok, ex_tar_in), ex_tar_out in val_dataset.take(1):\n",
    "  print(ex_context_tok[0, :10].numpy()) \n",
    "  print(ex_context_tok.shape)\n",
    "  print(ex_tar_in[0, :10].numpy()) \n",
    "  print(ex_tar_out[0, :10].numpy())\n",
    "  print(ex_tar_out.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd2a451f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
