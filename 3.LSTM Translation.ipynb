{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1d9451b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "#Legacy TensorFlow BackEnd\n",
    "os.environ['TF_USE_LEGACY_KERAS'] = '1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f2296651",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\yassi\\anaconda3\\Lib\\site-packages\\tf_keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import pathlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d265cd8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "config={\n",
    "    \"max_vocab_size\":5000,\n",
    "    \"max_length\":50,\n",
    "    \"BATCH_SIZE\":64,\n",
    "    \"Split_Ratio\":0.9,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3c07748b",
   "metadata": {},
   "outputs": [],
   "source": [
    "path_to_zip = tf.keras.utils.get_file(\n",
    "    'spa-eng.zip', origin='http://storage.googleapis.com/download.tensorflow.org/data/spa-eng.zip',\n",
    "    extract=True)\n",
    "\n",
    "path_to_file = pathlib.Path(path_to_zip).parent/'spa-eng/spa.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7ad086fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(path):\n",
    "  text = path.read_text(encoding='utf-8')\n",
    "\n",
    "  lines = text.splitlines()\n",
    "  pairs = [line.split('\\t') for line in lines]\n",
    "\n",
    "  context = np.array([context for target, context in pairs])\n",
    "  target = np.array([target for target, context in pairs])\n",
    "\n",
    "  return target, context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "164168d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "If you want to sound like a native speaker, you must be willing to practice saying the same sentence over and over in the same way that banjo players practice the same phrase over and over until they can play it correctly and at the desired tempo.\n"
     ]
    }
   ],
   "source": [
    "context_raw,target_raw = load_data(path_to_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bcdcd697",
   "metadata": {},
   "outputs": [],
   "source": [
    "split_idx = int(config[\"Split_Ratio\"] * len(target_raw))\n",
    "\n",
    "X_train = context_raw[:split_idx]\n",
    "y_train = target_raw[:split_idx]\n",
    "\n",
    "X_val = context_raw[split_idx:]\n",
    "y_val = target_raw[split_idx:]\n",
    "\n",
    "BUFFER_SIZE = len(X_train)\n",
    "\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices((X_train, y_train))\n",
    "train_dataset = train_dataset.shuffle(BUFFER_SIZE).batch(config['BATCH_SIZE'], drop_remainder=True).prefetch(tf.data.AUTOTUNE)\n",
    "\n",
    "# Validation dataset (no shuffle needed)\n",
    "val_dataset = tf.data.Dataset.from_tensor_slices((X_val, y_val))\n",
    "val_dataset = val_dataset.batch(config['BATCH_SIZE'], drop_remainder=True).prefetch(tf.data.AUTOTUNE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f6c30ba3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tf_lower_and_split_punct_w_special_tokens(text):\n",
    "  text = tf.strings.lower(text)\n",
    "  text = tf.strings.regex_replace(text, '[^ a-z.?!,多]', '')\n",
    "  text = tf.strings.regex_replace(text, '[.?!,多]', r' \\0 ')\n",
    "  text = tf.strings.strip(text)\n",
    "\n",
    "  text = tf.strings.join(['[START]', text, '[END]'], separator=' ')\n",
    "  return text\n",
    "\n",
    "def tf_lower_and_split_punct(text):\n",
    "  text = tf.strings.lower(text)\n",
    "  text = tf.strings.regex_replace(text, '[^ a-z.?!,多]', '')\n",
    "  text = tf.strings.regex_replace(text, '[.?!,多]', r' \\0 ')\n",
    "  text = tf.strings.strip(text)\n",
    "\n",
    "  return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cf8821db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\yassi\\anaconda3\\Lib\\site-packages\\tf_keras\\src\\backend.py:873: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "context_text_processor = tf.keras.layers.TextVectorization(\n",
    "    standardize=tf_lower_and_split_punct,\n",
    "    max_tokens=config['max_vocab_size'],\n",
    "    output_sequence_length = config['max_length'],\n",
    "    ragged=False)\n",
    "\n",
    "target_text_processor = tf.keras.layers.TextVectorization(\n",
    "    standardize=tf_lower_and_split_punct_w_special_tokens,\n",
    "    max_tokens=config['max_vocab_size'],\n",
    "    output_sequence_length = config['max_length'] + 1,\n",
    "    ragged=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "081e4f58",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\yassi\\anaconda3\\Lib\\site-packages\\tf_keras\\src\\utils\\tf_utils.py:492: The name tf.ragged.RaggedTensorValue is deprecated. Please use tf.compat.v1.ragged.RaggedTensorValue instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "context_text_processor.adapt(train_dataset.map(lambda context, target: context))\n",
    "target_text_processor.adapt(train_dataset.map(lambda context, target: target))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0c7e57b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_text(context, target):\n",
    "  context = context_text_processor(context)\n",
    "  target = target_text_processor(target)\n",
    "  targ_in = target[:,:-1]\n",
    "  targ_out = target[:,1:]\n",
    "  return (context, targ_in), targ_out\n",
    "\n",
    "\n",
    "train_dataset = train_dataset.map(process_text, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "val_dataset = val_dataset.map(process_text, num_parallel_calls = tf.data.AUTOTUNE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4fb76803",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "English vocab size: 5000\n",
      "French vocab size: 5000\n"
     ]
    }
   ],
   "source": [
    "def Build_Seq2Seq(max_length=157, vocab_size_en=10000, vocab_size_es=10000, embedding_dim=256, units=512):\n",
    "    encoder_input = tf.keras.layers.Input(shape=(max_length,), dtype=\"int32\", name='encoder_input')\n",
    "    \n",
    "    enc_emb = tf.keras.layers.Embedding(vocab_size_en, embedding_dim, mask_zero=True)(encoder_input)\n",
    "    enc_emb = tf.keras.layers.Dropout(0.2)(enc_emb)\n",
    "    \n",
    "    encoder = tf.keras.layers.Bidirectional(\n",
    "        tf.keras.layers.LSTM(\n",
    "            units, \n",
    "            return_sequences=True, \n",
    "            return_state=True,\n",
    "            name='encoder_lstm'\n",
    "        )\n",
    "    )\n",
    "    \n",
    "    encoder_outputs, forward_h, forward_c, backward_h, backward_c = encoder(enc_emb)\n",
    "    \n",
    "    encoder_state_h = tf.keras.layers.Concatenate(axis=-1)([forward_h, backward_h])\n",
    "    encoder_state_c = tf.keras.layers.Concatenate(axis=-1)([forward_c, backward_c])\n",
    "    \n",
    "    decoder_input = tf.keras.layers.Input(shape=(max_length,), dtype=\"int32\", name='decoder_input')\n",
    "    \n",
    "    dec_emb = tf.keras.layers.Embedding(vocab_size_es, embedding_dim, mask_zero=True)(decoder_input)\n",
    "    dec_emb = tf.keras.layers.Dropout(0.2)(dec_emb)\n",
    "    \n",
    "    decoder_outputs = tf.keras.layers.LSTM(\n",
    "        units * 2,\n",
    "        return_sequences=True,\n",
    "        return_state=False,\n",
    "        name='decoder_lstm'\n",
    "    )(dec_emb, initial_state=[encoder_state_h, encoder_state_c])\n",
    "    \n",
    "   \n",
    "    attention_output = tf.keras.layers.MultiHeadAttention(\n",
    "        num_heads=8,\n",
    "        key_dim=units * 2, \n",
    "        name='cross_attention'\n",
    "    )(\n",
    "        query=decoder_outputs, \n",
    "        value=encoder_outputs,  \n",
    "        key=encoder_outputs     \n",
    "    )\n",
    "    \n",
    "    decoder_combined = tf.keras.layers.Concatenate(axis=-1)([decoder_outputs, attention_output])\n",
    "    \n",
    "    outputs = tf.keras.layers.Dense(vocab_size_es, activation='softmax', name='output_dense')(decoder_combined)\n",
    "    \n",
    "    model = tf.keras.Model(inputs=[encoder_input, decoder_input], outputs=outputs, name='seq2seq')\n",
    "    return model\n",
    "vocab_size_en = context_text_processor.vocabulary_size()\n",
    "vocab_size_fr = target_text_processor.vocabulary_size()\n",
    "print(f\"English vocab size: {vocab_size_en}\")\n",
    "print(f\"French vocab size: {vocab_size_fr}\")\n",
    "\n",
    "model = Build_Seq2Seq(\n",
    "    max_length=config['max_length'], \n",
    "    vocab_size_en=vocab_size_en,\n",
    "    vocab_size_es=vocab_size_fr,\n",
    "    embedding_dim=256,\n",
    "    units=256  \n",
    "  \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e9f5b6de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"seq2seq\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                Output Shape                 Param #   Connected to                  \n",
      "==================================================================================================\n",
      " encoder_input (InputLayer)  [(None, 50)]                 0         []                            \n",
      "                                                                                                  \n",
      " embedding (Embedding)       (None, 50, 256)              1280000   ['encoder_input[0][0]']       \n",
      "                                                                                                  \n",
      " decoder_input (InputLayer)  [(None, 50)]                 0         []                            \n",
      "                                                                                                  \n",
      " dropout (Dropout)           (None, 50, 256)              0         ['embedding[0][0]']           \n",
      "                                                                                                  \n",
      " embedding_1 (Embedding)     (None, 50, 256)              1280000   ['decoder_input[0][0]']       \n",
      "                                                                                                  \n",
      " bidirectional (Bidirection  [(None, 50, 512),            1050624   ['dropout[0][0]']             \n",
      " al)                          (None, 256),                                                        \n",
      "                              (None, 256),                                                        \n",
      "                              (None, 256),                                                        \n",
      "                              (None, 256)]                                                        \n",
      "                                                                                                  \n",
      " dropout_1 (Dropout)         (None, 50, 256)              0         ['embedding_1[0][0]']         \n",
      "                                                                                                  \n",
      " concatenate (Concatenate)   (None, 512)                  0         ['bidirectional[0][1]',       \n",
      "                                                                     'bidirectional[0][3]']       \n",
      "                                                                                                  \n",
      " concatenate_1 (Concatenate  (None, 512)                  0         ['bidirectional[0][2]',       \n",
      " )                                                                   'bidirectional[0][4]']       \n",
      "                                                                                                  \n",
      " decoder_lstm (LSTM)         (None, 50, 512)              1574912   ['dropout_1[0][0]',           \n",
      "                                                                     'concatenate[0][0]',         \n",
      "                                                                     'concatenate_1[0][0]']       \n",
      "                                                                                                  \n",
      " cross_attention (MultiHead  (None, 50, 512)              8401408   ['decoder_lstm[0][0]',        \n",
      " Attention)                                                          'bidirectional[0][0]',       \n",
      "                                                                     'bidirectional[0][0]']       \n",
      "                                                                                                  \n",
      " concatenate_2 (Concatenate  (None, 50, 1024)             0         ['decoder_lstm[0][0]',        \n",
      " )                                                                   'cross_attention[0][0]']     \n",
      "                                                                                                  \n",
      " output_dense (Dense)        (None, 50, 5000)             5125000   ['concatenate_2[0][0]']       \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 18711944 (71.38 MB)\n",
      "Trainable params: 18711944 (71.38 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1d69b423",
   "metadata": {},
   "outputs": [],
   "source": [
    "def masked_loss(y_true, y_pred):\n",
    "    \n",
    "    loss_fn = tf.keras.losses.SparseCategoricalCrossentropy(\n",
    "        from_logits=False,\n",
    "        reduction='none'\n",
    "    )\n",
    "    loss = loss_fn(y_true, y_pred) \n",
    "\n",
    "    mask = tf.cast(y_true != 0, loss.dtype)\n",
    "    loss *= mask\n",
    "\n",
    "    return tf.reduce_sum(loss) / tf.reduce_sum(mask)\n",
    "\n",
    "\n",
    "def masked_acc(y_true, y_pred):\n",
    "    \n",
    "    y_pred = tf.argmax(y_pred, axis=-1)\n",
    "    y_pred = tf.cast(y_pred, y_true.dtype)\n",
    "\n",
    "    match = tf.cast(y_true == y_pred, tf.float32)\n",
    "    \n",
    "    mask = tf.cast(y_true != 0, tf.float32)\n",
    "\n",
    "    return tf.reduce_sum(match) / tf.reduce_sum(mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e11034e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam',\n",
    "              loss=masked_loss, \n",
    "              metrics=[masked_acc])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "149c8c07",
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(\n",
    "    train_dataset, \n",
    "    epochs=30,\n",
    "    validation_data=val_dataset,\n",
    "    callbacks=[\n",
    "        tf.keras.callbacks.EarlyStopping(\n",
    "            monitor='val_loss',\n",
    "            patience=5,\n",
    "            restore_best_weights=True\n",
    "        ),\n",
    "        tf.keras.callbacks.ReduceLROnPlateau(\n",
    "            monitor='val_loss',\n",
    "            factor=0.1,          \n",
    "            patience=3,           \n",
    "            min_lr=1e-7,          \n",
    "            verbose=1             \n",
    "        )\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ec7966d",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(\"BI-LSTM-Cross-ATT.keras\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "c9c68be4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:1: SyntaxWarning: invalid escape sequence '\\B'\n",
      "<>:1: SyntaxWarning: invalid escape sequence '\\B'\n",
      "C:\\Users\\yassi\\AppData\\Local\\Temp\\ipykernel_22036\\61559784.py:1: SyntaxWarning: invalid escape sequence '\\B'\n",
      "  model.load_weights(\"Model Weights\\BI-LSTM-Cross-ATT.h5\")\n"
     ]
    }
   ],
   "source": [
    "model.load_weights(\"Model Weights\\BI-LSTM-Cross-ATT.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68c64492",
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded_model = tf.keras.models.load_model(\"Model Weights\\BI-LSTM-Cross-ATT.keras\",compile=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "1cf4b6f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(64, 50)\n",
      "[   2   46 4227   14 1118   37    9  598  145    4]\n",
      "[  46 4227   14 1118   37    9  598  145    4    3]\n",
      "(64, 50)\n"
     ]
    }
   ],
   "source": [
    "(ex_context_tok, ex_tar_in), ex_tar_out = next(iter(val_dataset))\n",
    "print(ex_context_tok.shape)\n",
    "print(ex_tar_in[0, :10].numpy()) \n",
    "print(ex_tar_out[0, :10].numpy())\n",
    "print(ex_tar_out.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "8f28e0f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_initial_state(model, context, target_text_processor):\n",
    "    if len(context.shape) == 1:\n",
    "        context = tf.expand_dims(context, 0)\n",
    "    \n",
    "    batch_size = tf.shape(context)[0]\n",
    "    vocab = target_text_processor.get_vocabulary()\n",
    "    start_token = vocab.index('[START]')\n",
    "    \n",
    "    next_token = tf.fill([batch_size, 1], start_token)\n",
    "    done = tf.zeros([batch_size, 1], dtype=tf.bool)\n",
    "    \n",
    "    return next_token, done, context\n",
    "\n",
    "\n",
    "def get_next_token(model, context, next_token, done, state, target_text_processor, temperature=0.0):\n",
    "    vocab = target_text_processor.get_vocabulary()\n",
    "    end_token = vocab.index('[END]')\n",
    "    \n",
    "    padded_state = tf.pad(state, [[0, 0], [0, config['max_length'] - tf.shape(state)[1]]])[:, :config['max_length']]\n",
    "    logits = model.predict([context, padded_state], verbose=0)\n",
    "    last_logits = logits[:, tf.shape(state)[1] - 1, :]\n",
    "    \n",
    "    if temperature == 0.0:\n",
    "        next_token_id = tf.argmax(last_logits, axis=-1, output_type=tf.int32)\n",
    "    else:\n",
    "        next_token_id = tf.squeeze(tf.random.categorical(last_logits / temperature, 1, dtype=tf.int32), -1)\n",
    "    \n",
    "    next_token_id = tf.expand_dims(next_token_id, -1)\n",
    "    done = done | (next_token_id == end_token)\n",
    "    next_token_id = tf.where(done, tf.constant(0, dtype=tf.int32), next_token_id)\n",
    "    new_state = tf.concat([state, next_token_id], axis=1)\n",
    "    \n",
    "    return next_token_id, done, new_state\n",
    "\n",
    "\n",
    "def translate(model, spanish_text, target_text_processor, temperature=0.0):\n",
    "    if len(spanish_text.shape) == 1:\n",
    "        spanish_text = tf.expand_dims(spanish_text, 0)\n",
    "    \n",
    "    next_token, done, context = get_initial_state(model, spanish_text, target_text_processor)\n",
    "    state = next_token\n",
    "    tokens = []\n",
    "    \n",
    "    for n in range(config['max_length']):\n",
    "        next_token, done, state = get_next_token(model, context, next_token, done, state, target_text_processor, temperature)\n",
    "        tokens.append(next_token)\n",
    "        if tf.reduce_all(done):\n",
    "            break\n",
    "    \n",
    "    tokens = tf.concat(tokens, axis=-1)\n",
    "    vocab = target_text_processor.get_vocabulary()\n",
    "    \n",
    "    words = []\n",
    "    for token_id in tokens[0].numpy():\n",
    "        if token_id == 0:\n",
    "            break\n",
    "        word = vocab[token_id]\n",
    "        if word == '[END]':\n",
    "            break\n",
    "        if word not in ['[START]', '[UNK]', '']:\n",
    "            words.append(word)\n",
    "    \n",
    "    return ' '.join(words)\n",
    "\n",
    "\n",
    "def compare_translations(model, spanish_input, target_out, context_text_processor, target_text_processor, n=5):\n",
    "    spanish_vocab = context_text_processor.get_vocabulary()\n",
    "    english_vocab = target_text_processor.get_vocabulary()\n",
    "    \n",
    "    for i in range(min(n, spanish_input.shape[0])):\n",
    "        # Spanish input\n",
    "        sp_words = [spanish_vocab[t] for t in spanish_input[i].numpy() if t > 0]\n",
    "        spanish = ' '.join(sp_words)\n",
    "        \n",
    "        # Ground truth English\n",
    "        gt_words = [english_vocab[t] for t in target_out[i].numpy() \n",
    "                    if t > 0 and english_vocab[t] not in ['[START]', '[END]']]\n",
    "        ground_truth = ' '.join(gt_words)\n",
    "        \n",
    "        # Model translation\n",
    "        model_output = translate(model, spanish_input[i], target_text_processor)\n",
    "        \n",
    "        print(f\"\\n{i+1}. English: {spanish}\")\n",
    "        print(f\"   GROUNDTRUTH: {ground_truth}\")\n",
    "        print(f\"   TRANSLATION: {model_output}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "932352fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "1. English: this is the same necklace that i lost yesterday .\n",
      "   GROUNDTRUTH: este collar es igual al que perd ayer .\n",
      "   TRANSLATION: eso es no ingls de gatos tres .\n",
      "\n",
      "2. English: this is the strongest dog that i have ever seen .\n",
      "   GROUNDTRUTH: este es el perro ms fuerte que jams haya visto .\n",
      "   TRANSLATION: se es para tom para de si auto en el estado .\n",
      "\n",
      "3. English: this is your last chance to spend time with tom .\n",
      "   GROUNDTRUTH: esta es tu ltima oportunidad de pasar tiempo con tom .\n",
      "   TRANSLATION: se es te pasar cuatro tom edad tiempo le que .\n",
      "\n",
      "4. English: this material will stand up to lots of [UNK] .\n",
      "   GROUNDTRUTH: este material aguantar un montn de [UNK] .\n",
      "   TRANSLATION: se vino ? tom .\n",
      "\n",
      "5. English: this medicine should be taken every three hours .\n",
      "   GROUNDTRUTH: este medicamento debe ser tomado cada tres horas .\n",
      "   TRANSLATION: eso trato decir aos siente parece gato .\n",
      "\n",
      "6. English: this morning the teacher got very angry with me .\n",
      "   GROUNDTRUTH: la profesora se enoj mucho conmigo esta maana .\n",
      "   TRANSLATION: se nunca se ? ven tom rompi tiene gracias .\n",
      "\n",
      "7. English: this mountain is covered in snow all year round .\n",
      "   GROUNDTRUTH: esta montaa est cubierta de nieve durante todo el ao .\n",
      "   TRANSLATION: se llevo se fingi tom camisa s su .\n",
      "\n",
      "8. English: this picture reminds me of when i was a student .\n",
      "   GROUNDTRUTH: esta foto me recuerda a mi poca de estudiante .\n",
      "   TRANSLATION: se doctor l revistas ha al era dame .\n",
      "\n",
      "9. English: this [UNK] flew from san francisco to new york .\n",
      "   GROUNDTRUTH: esta [UNK] vol de san [UNK] a nueva york .\n",
      "   TRANSLATION: eso final ? el ? el amigos tom encontr .\n",
      "\n",
      "10. English: this problem may be solved in a [UNK] of ways .\n",
      "   GROUNDTRUTH: este problema puede ser resuelto de varias [UNK] .\n",
      "   TRANSLATION: bien de eso hermana gustan tom sencillamente en un tom el .\n",
      "\n",
      "1. English: tom doesnt even know how to start a lawn [UNK] .\n",
      "   GROUNDTRUTH: tom ni siquiera sabe como encender una [UNK] de csped .\n",
      "   TRANSLATION: que pronto pequeo quiere ? puede coche un .\n",
      "\n",
      "2. English: tom doesnt even know how to write his own name .\n",
      "   GROUNDTRUTH: tom ni siquiera sabe cmo escribir su propio nombre .\n",
      "   TRANSLATION: que pronto pequeo quiere y olvid mi amigo .\n",
      "\n",
      "3. English: tom doesnt feel like eating anything right now .\n",
      "   GROUNDTRUTH: tom no tiene ganas de comer nada ahora mismo .\n",
      "   TRANSLATION: que a muy importa tom haber todos t ingls .\n",
      "\n",
      "4. English: tom doesnt have a ticket for [UNK] concert .\n",
      "   GROUNDTRUTH: tom no tiene [UNK] para el concierto de esta noche .\n",
      "   TRANSLATION: que a muy un domingo ltima .\n",
      "\n",
      "5. English: tom doesnt have any idea what mary is thinking .\n",
      "   GROUNDTRUTH: tom no tiene ni idea de en qu est pensando mary .\n",
      "   TRANSLATION: que a muy muchos tom lo de con es estaban ser .\n",
      "\n",
      "6. English: tom doesnt have the [UNK] of a good leader .\n",
      "   GROUNDTRUTH: tom no tiene las [UNK] de un buen lder .\n",
      "   TRANSLATION: que a muy el tom un errores tom haber .\n",
      "\n",
      "7. English: tom doesnt have to hide his feelings from mary .\n",
      "   GROUNDTRUTH: tom no tiene que ocultar sus sentimientos por mary .\n",
      "   TRANSLATION: que a muy de millas trabajo encantan tom con .\n",
      "\n",
      "8. English: tom doesnt know anything about classical music .\n",
      "   GROUNDTRUTH: tom no sabe nada de msica clsica .\n",
      "   TRANSLATION: que a quiere todos trabajar tom el aprender record .\n",
      "\n",
      "9. English: tom doesnt know what this is called in english .\n",
      "   GROUNDTRUTH: tom no sabe como se llama esto en ingls .\n",
      "   TRANSLATION: que a quiere de casa me rico en nuevo .\n",
      "\n",
      "10. English: tom doesnt know why mary and john are fighting .\n",
      "   GROUNDTRUTH: tom no sabe por qu mary y john estn peleando .\n",
      "   TRANSLATION: que a quiere por de con yo finalmente favor huele .\n"
     ]
    }
   ],
   "source": [
    "\n",
    "val_iter = iter(val_dataset)\n",
    "\n",
    "(ex_context_tok, ex_tar_in), ex_tar_out = next(val_iter)\n",
    "compare_translations(model, ex_context_tok, ex_tar_out, context_text_processor, target_text_processor, n=10)\n",
    "\n",
    "\n",
    "(ex_context_tok, ex_tar_in), ex_tar_out = next(val_iter)\n",
    "compare_translations(model, ex_context_tok, ex_tar_out, context_text_processor, target_text_processor, n=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a81655c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
