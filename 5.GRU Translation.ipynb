{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.12.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":692482,"sourceType":"modelInstanceVersion","isSourceIdPinned":true,"modelInstanceId":525061,"modelId":539100}],"dockerImageVersionId":31236,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import os\n#Legacy TensorFlow BackEnd\nos.environ['TF_USE_LEGACY_KERAS'] = '1'","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-12-19T16:23:14.782735Z","iopub.execute_input":"2025-12-19T16:23:14.783266Z","iopub.status.idle":"2025-12-19T16:23:14.789697Z","shell.execute_reply.started":"2025-12-19T16:23:14.783236Z","shell.execute_reply":"2025-12-19T16:23:14.788796Z"}},"outputs":[],"execution_count":1},{"cell_type":"code","source":"!pip install sacrebleu --quiet\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-19T16:23:14.941935Z","iopub.execute_input":"2025-12-19T16:23:14.942173Z","iopub.status.idle":"2025-12-19T16:23:19.446899Z","shell.execute_reply.started":"2025-12-19T16:23:14.942150Z","shell.execute_reply":"2025-12-19T16:23:19.446221Z"}},"outputs":[{"name":"stdout","text":"\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m51.8/51.8 kB\u001b[0m \u001b[31m1.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m104.1/104.1 kB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25h","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"import numpy as np\nimport matplotlib.pyplot as plt\nimport tensorflow as tf\nfrom tensorflow.keras.preprocessing.text import Tokenizer\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\nimport pathlib\nimport tensorflow_text as tf_text\nfrom tqdm import tqdm\nimport sacrebleu\nimport pickle","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-19T16:23:46.194265Z","iopub.execute_input":"2025-12-19T16:23:46.194844Z","iopub.status.idle":"2025-12-19T16:23:46.198647Z","shell.execute_reply.started":"2025-12-19T16:23:46.194814Z","shell.execute_reply":"2025-12-19T16:23:46.198008Z"}},"outputs":[],"execution_count":17},{"cell_type":"code","source":"strategy = tf.distribute.MirroredStrategy()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-19T16:23:34.196015Z","iopub.execute_input":"2025-12-19T16:23:34.196538Z","iopub.status.idle":"2025-12-19T16:23:35.146357Z","shell.execute_reply.started":"2025-12-19T16:23:34.196507Z","shell.execute_reply":"2025-12-19T16:23:35.145779Z"}},"outputs":[{"name":"stdout","text":"INFO:tensorflow:Using MirroredStrategy with devices ('/job:localhost/replica:0/task:0/device:GPU:0', '/job:localhost/replica:0/task:0/device:GPU:1')\n","output_type":"stream"},{"name":"stderr","text":"I0000 00:00:1766161415.104204      55 gpu_device.cc:2019] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 13942 MB memory:  -> device: 0, name: Tesla T4, pci bus id: 0000:00:04.0, compute capability: 7.5\nI0000 00:00:1766161415.104901      55 gpu_device.cc:2019] Created device /job:localhost/replica:0/task:0/device:GPU:1 with 13942 MB memory:  -> device: 1, name: Tesla T4, pci bus id: 0000:00:05.0, compute capability: 7.5\n","output_type":"stream"}],"execution_count":4},{"cell_type":"code","source":"print('Number of devices: {}'.format(strategy.num_replicas_in_sync))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-19T16:23:35.148196Z","iopub.execute_input":"2025-12-19T16:23:35.148490Z","iopub.status.idle":"2025-12-19T16:23:35.152287Z","shell.execute_reply.started":"2025-12-19T16:23:35.148466Z","shell.execute_reply":"2025-12-19T16:23:35.151666Z"}},"outputs":[{"name":"stdout","text":"Number of devices: 2\n","output_type":"stream"}],"execution_count":5},{"cell_type":"code","source":"config = {\n    \"learning_rate\": 1e-4,\n    \"batch_size\": 256,\n    \"epochs\": 20,\n    \"max_vocab_size\":5000,\n    \"max_length\":50\n}\n\nGLOBAL_BATCH = config['batch_size'] * strategy.num_replicas_in_sync\nLEARNING_RATE = config['learning_rate']\nEPOCHS = config['epochs']","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-19T16:23:35.153112Z","iopub.execute_input":"2025-12-19T16:23:35.153400Z","iopub.status.idle":"2025-12-19T16:23:35.165309Z","shell.execute_reply.started":"2025-12-19T16:23:35.153370Z","shell.execute_reply":"2025-12-19T16:23:35.164752Z"}},"outputs":[],"execution_count":6},{"cell_type":"code","source":"path_to_zip = tf.keras.utils.get_file(\n    'spa-eng.zip', origin='http://storage.googleapis.com/download.tensorflow.org/data/spa-eng.zip',\n    extract=True)\n\npath_to_file = pathlib.Path(path_to_zip).parent/'spa-eng/spa.txt'","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-19T16:23:35.166133Z","iopub.execute_input":"2025-12-19T16:23:35.166357Z","iopub.status.idle":"2025-12-19T16:23:35.308591Z","shell.execute_reply.started":"2025-12-19T16:23:35.166337Z","shell.execute_reply":"2025-12-19T16:23:35.308083Z"}},"outputs":[{"name":"stdout","text":"Downloading data from http://storage.googleapis.com/download.tensorflow.org/data/spa-eng.zip\n2638744/2638744 [==============================] - 0s 0us/step\n","output_type":"stream"}],"execution_count":7},{"cell_type":"code","source":"def load_data(path):\n  text = path.read_text(encoding='utf-8')\n\n  lines = text.splitlines()\n  pairs = [line.split('\\t') for line in lines]\n\n  context = np.array([context for target, context in pairs])\n  target = np.array([target for target, context in pairs])\n\n  return target, context","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-19T16:23:35.309422Z","iopub.execute_input":"2025-12-19T16:23:35.309691Z","iopub.status.idle":"2025-12-19T16:23:35.314828Z","shell.execute_reply.started":"2025-12-19T16:23:35.309652Z","shell.execute_reply":"2025-12-19T16:23:35.314288Z"}},"outputs":[],"execution_count":8},{"cell_type":"code","source":"context_raw,target_raw = load_data(path_to_file)\nprint(context_raw[-1])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-19T16:23:35.315758Z","iopub.execute_input":"2025-12-19T16:23:35.315983Z","iopub.status.idle":"2025-12-19T16:23:35.884694Z","shell.execute_reply.started":"2025-12-19T16:23:35.315963Z","shell.execute_reply":"2025-12-19T16:23:35.884104Z"}},"outputs":[{"name":"stdout","text":"If you want to sound like a native speaker, you must be willing to practice saying the same sentence over and over in the same way that banjo players practice the same phrase over and over until they can play it correctly and at the desired tempo.\n","output_type":"stream"}],"execution_count":9},{"cell_type":"code","source":"print(target_raw[-1])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-19T16:23:35.885559Z","iopub.execute_input":"2025-12-19T16:23:35.885844Z","iopub.status.idle":"2025-12-19T16:23:35.889599Z","shell.execute_reply.started":"2025-12-19T16:23:35.885822Z","shell.execute_reply":"2025-12-19T16:23:35.888976Z"}},"outputs":[{"name":"stdout","text":"Si quieres sonar como un hablante nativo, debes estar dispuesto a practicar diciendo la misma frase una y otra vez de la misma manera en que un músico de banjo practica el mismo fraseo una y otra vez hasta que lo puedan tocar correctamente y en el tiempo esperado.\n","output_type":"stream"}],"execution_count":10},{"cell_type":"code","source":"split_idx = int(0.9 * len(target_raw))\n\nX_train = context_raw[:split_idx]\ny_train = target_raw[:split_idx]\n\nX_val = context_raw[split_idx:]\ny_val = target_raw[split_idx:]\n\nBUFFER_SIZE = len(X_train)\n\ntrain_dataset = tf.data.Dataset.from_tensor_slices((X_train, y_train))\ntrain_dataset = train_dataset.shuffle(BUFFER_SIZE).batch(GLOBAL_BATCH, drop_remainder=True).prefetch(tf.data.AUTOTUNE)\n\n# Validation dataset (no shuffle needed)\nval_dataset = tf.data.Dataset.from_tensor_slices((X_val, y_val))\nval_dataset = val_dataset.batch(GLOBAL_BATCH, drop_remainder=True).prefetch(tf.data.AUTOTUNE)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-19T16:23:35.891292Z","iopub.execute_input":"2025-12-19T16:23:35.891585Z","iopub.status.idle":"2025-12-19T16:23:36.226356Z","shell.execute_reply.started":"2025-12-19T16:23:35.891564Z","shell.execute_reply":"2025-12-19T16:23:36.225788Z"}},"outputs":[],"execution_count":11},{"cell_type":"code","source":"TRAIN_STEPS = tf.data.experimental.cardinality(train_dataset).numpy()\nVAL_STEPS = tf.data.experimental.cardinality(val_dataset).numpy()\n\nprint(TRAIN_STEPS, VAL_STEPS)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-19T16:23:36.227193Z","iopub.execute_input":"2025-12-19T16:23:36.227426Z","iopub.status.idle":"2025-12-19T16:23:36.232448Z","shell.execute_reply.started":"2025-12-19T16:23:36.227404Z","shell.execute_reply":"2025-12-19T16:23:36.231865Z"}},"outputs":[{"name":"stdout","text":"209 23\n","output_type":"stream"}],"execution_count":12},{"cell_type":"code","source":"idx = np.random.randint(0,63)\ncontext,target = next(iter(val_dataset))\nprint(f\"Input:{context[idx].numpy().decode('utf-8')}\")\nprint(f\"Target:{target[idx].numpy().decode('utf-8')}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-19T16:23:36.233197Z","iopub.execute_input":"2025-12-19T16:23:36.233425Z","iopub.status.idle":"2025-12-19T16:23:36.287740Z","shell.execute_reply.started":"2025-12-19T16:23:36.233395Z","shell.execute_reply":"2025-12-19T16:23:36.287183Z"}},"outputs":[{"name":"stdout","text":"Input:Tom and Mary usually speak French to each other.\nTarget:Tom y María normalmente hablan francés entre ellos.\n","output_type":"stream"}],"execution_count":13},{"cell_type":"code","source":"def tf_lower_and_split_punct_w_special_tokens(text):\n  text = tf_text.normalize_utf8(text, 'NFKD')\n  text = tf.strings.lower(text)\n  text = tf.strings.regex_replace(text, '[^ a-z.?!,¿]', '')\n  text = tf.strings.regex_replace(text, '[.?!,¿]', r' \\0 ')\n  text = tf.strings.strip(text)\n\n  text = tf.strings.join(['[START]', text, '[END]'], separator=' ')\n  return text\n\ndef tf_lower_and_split_punct(text):\n  text = tf_text.normalize_utf8(text, 'NFKD')\n  text = tf.strings.lower(text)\n  text = tf.strings.regex_replace(text, '[^ a-z.?!,¿]', '')\n  text = tf.strings.regex_replace(text, '[.?!,¿]', r' \\0 ')\n  text = tf.strings.strip(text)\n\n  return text","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-19T16:23:36.288478Z","iopub.execute_input":"2025-12-19T16:23:36.288733Z","iopub.status.idle":"2025-12-19T16:23:36.294048Z","shell.execute_reply.started":"2025-12-19T16:23:36.288692Z","shell.execute_reply":"2025-12-19T16:23:36.293225Z"}},"outputs":[],"execution_count":14},{"cell_type":"code","source":"print(target[idx].numpy().decode())\nprint(tf_lower_and_split_punct(target[idx]).numpy().decode())\nprint(tf_lower_and_split_punct_w_special_tokens(target[idx]).numpy().decode())","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-19T16:23:36.294976Z","iopub.execute_input":"2025-12-19T16:23:36.295211Z","iopub.status.idle":"2025-12-19T16:23:36.317207Z","shell.execute_reply.started":"2025-12-19T16:23:36.295192Z","shell.execute_reply":"2025-12-19T16:23:36.316529Z"}},"outputs":[{"name":"stdout","text":"Tom y María normalmente hablan francés entre ellos.\ntom y maria normalmente hablan frances entre ellos .\n[START] tom y maria normalmente hablan frances entre ellos . [END]\n","output_type":"stream"}],"execution_count":15},{"cell_type":"code","source":"with open('/kaggle/input/vocab/tensorflow2/default/1/context_vocab.pkl', 'rb') as f:\n    context_vocab = pickle.load(f)\n\nwith open('/kaggle/input/vocab/tensorflow2/default/1/target_vocab.pkl', 'rb') as f:\n    target_vocab = pickle.load(f)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-19T16:23:49.546184Z","iopub.execute_input":"2025-12-19T16:23:49.546874Z","iopub.status.idle":"2025-12-19T16:23:49.584675Z","shell.execute_reply.started":"2025-12-19T16:23:49.546842Z","shell.execute_reply":"2025-12-19T16:23:49.584146Z"}},"outputs":[],"execution_count":18},{"cell_type":"code","source":"context_text_processor = tf.keras.layers.TextVectorization(\n    standardize = tf_lower_and_split_punct,\n    max_tokens = config['max_vocab_size'],\n    output_sequence_length = config['max_length'],\n    vocabulary = context_vocab,\n    ragged = False)\n\ntarget_text_processor = tf.keras.layers.TextVectorization(\n    standardize = tf_lower_and_split_punct_w_special_tokens,\n    max_tokens = config['max_vocab_size'],\n    output_sequence_length = config['max_length'] + 1,\n    vocabulary = target_vocab,\n    ragged = False)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-19T16:23:57.105589Z","iopub.execute_input":"2025-12-19T16:23:57.106224Z","iopub.status.idle":"2025-12-19T16:23:57.199035Z","shell.execute_reply.started":"2025-12-19T16:23:57.106194Z","shell.execute_reply":"2025-12-19T16:23:57.198396Z"}},"outputs":[],"execution_count":19},{"cell_type":"code","source":"# context_text_processor.adapt(train_dataset.map(lambda context, target: context))\n\nprint(context_text_processor.get_vocabulary()[:10])\n\n\n# target_text_processor.adapt(train_dataset.map(lambda context, target: target))\ntarget_text_processor.get_vocabulary()[:10]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-19T16:24:13.986215Z","iopub.execute_input":"2025-12-19T16:24:13.986518Z","iopub.status.idle":"2025-12-19T16:24:14.011337Z","shell.execute_reply.started":"2025-12-19T16:24:13.986494Z","shell.execute_reply":"2025-12-19T16:24:14.010782Z"}},"outputs":[{"name":"stdout","text":"['', '[UNK]', np.str_('.'), np.str_('i'), np.str_('the'), np.str_('to'), np.str_('you'), np.str_('tom'), np.str_('?'), np.str_('a')]\n","output_type":"stream"},{"execution_count":20,"output_type":"execute_result","data":{"text/plain":"['',\n '[UNK]',\n np.str_('[START]'),\n np.str_('[END]'),\n np.str_('.'),\n np.str_('que'),\n np.str_('el'),\n np.str_('de'),\n np.str_('no'),\n np.str_('tom')]"},"metadata":{}}],"execution_count":20},{"cell_type":"code","source":"def process_text(context, target):\n  context = context_text_processor(context)\n  target = target_text_processor(target)\n  targ_in = target[:,:-1]\n  targ_out = target[:,1:]\n  return (context, targ_in), targ_out\n\n\ntrain_dataset = train_dataset.map(process_text, num_parallel_calls=tf.data.AUTOTUNE)\nval_dataset = val_dataset.map(process_text, num_parallel_calls = tf.data.AUTOTUNE)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-19T16:24:19.497753Z","iopub.execute_input":"2025-12-19T16:24:19.498497Z","iopub.status.idle":"2025-12-19T16:24:19.665851Z","shell.execute_reply.started":"2025-12-19T16:24:19.498468Z","shell.execute_reply":"2025-12-19T16:24:19.665296Z"}},"outputs":[],"execution_count":21},{"cell_type":"code","source":"batch = next(iter(val_dataset))\nreplica_batch = strategy.experimental_local_results(batch)[0]\n(ex_context_tok, ex_tar_in), ex_tar_out = replica_batch\nprint(ex_context_tok[0, :10].numpy()) \nprint(ex_context_tok.shape)\nprint(ex_tar_in[0, :10].numpy()) \nprint(ex_tar_out[0, :10].numpy())\nprint(ex_tar_out.shape)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-19T16:24:19.670277Z","iopub.execute_input":"2025-12-19T16:24:19.670499Z","iopub.status.idle":"2025-12-19T16:24:19.747706Z","shell.execute_reply.started":"2025-12-19T16:24:19.670479Z","shell.execute_reply":"2025-12-19T16:24:19.747093Z"}},"outputs":[{"name":"stdout","text":"[  18   10    4  297 4408   19    3  212  179    2]\n(512, 50)\n[   2   42 4238   14 1118   36    5  592  137    4]\n[  42 4238   14 1118   36    5  592  137    4    3]\n(512, 50)\n","output_type":"stream"}],"execution_count":22},{"cell_type":"code","source":"def Build_Seq2Seq(\n    max_length=157,\n    vocab_size_en=10000,\n    vocab_size_es=10000,\n    embedding_dim=256,\n    units=256\n):\n    # ================= Encoder =================\n    encoder_input = tf.keras.layers.Input(\n        shape=(max_length,), dtype=\"int32\", name=\"encoder_input\"\n    )\n\n    enc_emb = tf.keras.layers.Embedding(\n        vocab_size_en, embedding_dim, mask_zero=True\n    )(encoder_input)\n    enc_emb = tf.keras.layers.Dropout(0.2)(enc_emb)\n\n    encoder = tf.keras.layers.Bidirectional(\n        tf.keras.layers.GRU(\n            units,\n            return_sequences=True,\n            return_state=True,\n            name=\"encoder_gru\"\n        )\n    )\n\n    encoder_outputs, forward_h, backward_h = encoder(enc_emb)\n\n    # Concatenate forward + backward hidden states\n    encoder_state_h = tf.keras.layers.Concatenate(axis=-1)(\n        [forward_h, backward_h]\n    )  # shape = (batch, units*2)\n\n    # ================= Decoder =================\n    decoder_input = tf.keras.layers.Input(\n        shape=(max_length,), dtype=\"int32\", name=\"decoder_input\"\n    )\n\n    dec_emb = tf.keras.layers.Embedding(\n        vocab_size_es, embedding_dim, mask_zero=True\n    )(decoder_input)\n    dec_emb = tf.keras.layers.Dropout(0.2)(dec_emb)\n\n    decoder_outputs = tf.keras.layers.GRU(\n        units * 2,                 # MUST match encoder_state_h\n        return_sequences=True,\n        name=\"decoder_gru\"\n    )(dec_emb, initial_state=[encoder_state_h])\n\n    # ================= Cross Attention =================\n    attention_output = tf.keras.layers.MultiHeadAttention(\n        num_heads=8,\n        key_dim=units * 2,\n        name=\"cross_attention\"\n    )(\n        query=decoder_outputs,\n        value=encoder_outputs,\n        key=encoder_outputs\n    )\n\n    # ================= Output =================\n    decoder_combined = tf.keras.layers.Concatenate(axis=-1)(\n        [decoder_outputs, attention_output]\n    )\n\n    outputs = tf.keras.layers.Dense(\n        vocab_size_es, activation=\"softmax\", name=\"output_dense\"\n    )(decoder_combined)\n\n    model = tf.keras.Model(\n        inputs=[encoder_input, decoder_input],\n        outputs=outputs,\n        name=\"seq2seq_gru\"\n    )\n\n    return model\n\n\nvocab_size_en = context_text_processor.vocabulary_size()\nvocab_size_fr = target_text_processor.vocabulary_size()\nprint(f\"English vocab size: {vocab_size_en}\")\nprint(f\"French vocab size: {vocab_size_fr}\")\n\nwith strategy.scope():\n    model = Build_Seq2Seq(\n        max_length=config['max_length'], \n        vocab_size_en=vocab_size_en,\n        vocab_size_es=vocab_size_fr,\n        embedding_dim=256,\n        units=256  \n    )\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-19T16:24:24.929056Z","iopub.execute_input":"2025-12-19T16:24:24.929599Z","iopub.status.idle":"2025-12-19T16:24:28.165606Z","shell.execute_reply.started":"2025-12-19T16:24:24.929572Z","shell.execute_reply":"2025-12-19T16:24:28.165017Z"}},"outputs":[{"name":"stdout","text":"English vocab size: 5000\nFrench vocab size: 5000\n","output_type":"stream"}],"execution_count":23},{"cell_type":"code","source":"model.summary()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-19T16:24:29.181951Z","iopub.execute_input":"2025-12-19T16:24:29.182271Z","iopub.status.idle":"2025-12-19T16:24:29.213938Z","shell.execute_reply.started":"2025-12-19T16:24:29.182246Z","shell.execute_reply":"2025-12-19T16:24:29.213412Z"}},"outputs":[{"name":"stdout","text":"Model: \"seq2seq_gru\"\n__________________________________________________________________________________________________\n Layer (type)                Output Shape                 Param #   Connected to                  \n==================================================================================================\n encoder_input (InputLayer)  [(None, 50)]                 0         []                            \n                                                                                                  \n embedding (Embedding)       (None, 50, 256)              1280000   ['encoder_input[0][0]']       \n                                                                                                  \n decoder_input (InputLayer)  [(None, 50)]                 0         []                            \n                                                                                                  \n dropout (Dropout)           (None, 50, 256)              0         ['embedding[0][0]']           \n                                                                                                  \n embedding_1 (Embedding)     (None, 50, 256)              1280000   ['decoder_input[0][0]']       \n                                                                                                  \n bidirectional (Bidirection  [(None, 50, 512),            789504    ['dropout[0][0]']             \n al)                          (None, 256),                                                        \n                              (None, 256)]                                                        \n                                                                                                  \n dropout_1 (Dropout)         (None, 50, 256)              0         ['embedding_1[0][0]']         \n                                                                                                  \n concatenate (Concatenate)   (None, 512)                  0         ['bidirectional[0][1]',       \n                                                                     'bidirectional[0][2]']       \n                                                                                                  \n decoder_gru (GRU)           (None, 50, 512)              1182720   ['dropout_1[0][0]',           \n                                                                     'concatenate[0][0]']         \n                                                                                                  \n cross_attention (MultiHead  (None, 50, 512)              8401408   ['decoder_gru[0][0]',         \n Attention)                                                          'bidirectional[0][0]',       \n                                                                     'bidirectional[0][0]']       \n                                                                                                  \n concatenate_1 (Concatenate  (None, 50, 1024)             0         ['decoder_gru[0][0]',         \n )                                                                   'cross_attention[0][0]']     \n                                                                                                  \n output_dense (Dense)        (None, 50, 5000)             5125000   ['concatenate_1[0][0]']       \n                                                                                                  \n==================================================================================================\nTotal params: 18058632 (68.89 MB)\nTrainable params: 18058632 (68.89 MB)\nNon-trainable params: 0 (0.00 Byte)\n__________________________________________________________________________________________________\n","output_type":"stream"}],"execution_count":24},{"cell_type":"code","source":"def masked_loss(y_true, y_pred):\n    \n    loss_fn = tf.keras.losses.SparseCategoricalCrossentropy(\n        from_logits=False,\n        reduction='none'\n    )\n    loss = loss_fn(y_true, y_pred) \n\n    mask = tf.cast(y_true != 0, loss.dtype)\n    loss *= mask\n\n    return tf.reduce_sum(loss) / tf.reduce_sum(mask)\n\n\ndef masked_acc(y_true, y_pred):\n    \n    y_pred = tf.argmax(y_pred, axis=-1)\n    y_pred = tf.cast(y_pred, y_true.dtype)\n\n    match = tf.cast(y_true == y_pred, tf.float32)\n    \n    mask = tf.cast(y_true != 0, tf.float32)\n\n    return tf.reduce_sum(match) / tf.reduce_sum(mask)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-19T16:24:31.074306Z","iopub.execute_input":"2025-12-19T16:24:31.074651Z","iopub.status.idle":"2025-12-19T16:24:31.080206Z","shell.execute_reply.started":"2025-12-19T16:24:31.074624Z","shell.execute_reply":"2025-12-19T16:24:31.079416Z"}},"outputs":[],"execution_count":25},{"cell_type":"code","source":"with strategy.scope():\n    model.compile(optimizer='adam',\n                  loss=masked_loss, \n                  metrics=[masked_acc])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-19T16:24:31.618600Z","iopub.execute_input":"2025-12-19T16:24:31.619237Z","iopub.status.idle":"2025-12-19T16:24:31.643999Z","shell.execute_reply.started":"2025-12-19T16:24:31.619210Z","shell.execute_reply":"2025-12-19T16:24:31.643420Z"}},"outputs":[],"execution_count":26},{"cell_type":"code","source":"history = model.fit(\n    train_dataset, \n    epochs=30,\n    validation_data=val_dataset,\n    callbacks=[\n        tf.keras.callbacks.EarlyStopping(\n            monitor='val_loss',\n            patience=5,\n            restore_best_weights=True\n        ),\n        tf.keras.callbacks.ReduceLROnPlateau(\n            monitor='val_loss',\n            factor=0.1,          \n            patience=3,           \n            min_lr=1e-7,          \n            verbose=1             \n        )\n    ]\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-19T16:24:33.444643Z","iopub.execute_input":"2025-12-19T16:24:33.445445Z","iopub.status.idle":"2025-12-19T16:49:36.474433Z","shell.execute_reply.started":"2025-12-19T16:24:33.445417Z","shell.execute_reply":"2025-12-19T16:49:36.473701Z"}},"outputs":[{"name":"stdout","text":"Epoch 1/30\nINFO:tensorflow:Collective all_reduce tensors: 19 all_reduces, num_devices = 2, group_size = 2, implementation = CommunicationImplementation.NCCL, num_packs = 1\nINFO:tensorflow:Collective all_reduce IndexedSlices: 2 all_reduces, num_devices =2, group_size = 2, implementation = CommunicationImplementation.NCCL\nINFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\nINFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\nINFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\nINFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\nINFO:tensorflow:Collective all_reduce tensors: 19 all_reduces, num_devices = 2, group_size = 2, implementation = CommunicationImplementation.NCCL, num_packs = 1\nINFO:tensorflow:Collective all_reduce IndexedSlices: 2 all_reduces, num_devices =2, group_size = 2, implementation = CommunicationImplementation.NCCL\nINFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\nINFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\nINFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\nINFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n","output_type":"stream"},{"name":"stderr","text":"I0000 00:00:1766161493.279102     132 cuda_dnn.cc:529] Loaded cuDNN version 91002\nI0000 00:00:1766161493.279121     129 cuda_dnn.cc:529] Loaded cuDNN version 91002\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nI0000 00:00:1766161495.383004     131 service.cc:152] XLA service 0x7c5ad38df660 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\nI0000 00:00:1766161495.383047     131 service.cc:160]   StreamExecutor device (0): Tesla T4, Compute Capability 7.5\nI0000 00:00:1766161495.383056     131 service.cc:160]   StreamExecutor device (1): Tesla T4, Compute Capability 7.5\nI0000 00:00:1766161495.772253     131 device_compiler.h:188] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n","output_type":"stream"},{"name":"stdout","text":"209/209 [==============================] - ETA: 0s - loss: 4.2963 - masked_acc: 0.3224INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\nINFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n209/209 [==============================] - 141s 560ms/step - loss: 4.2963 - masked_acc: 0.3224 - val_loss: 3.8985 - val_masked_acc: 0.3308 - lr: 0.0010\nEpoch 2/30\n209/209 [==============================] - 95s 453ms/step - loss: 2.2456 - masked_acc: 0.5854 - val_loss: 2.5287 - val_masked_acc: 0.5240 - lr: 0.0010\nEpoch 3/30\n209/209 [==============================] - 92s 440ms/step - loss: 1.4260 - masked_acc: 0.7042 - val_loss: 2.0940 - val_masked_acc: 0.5764 - lr: 0.0010\nEpoch 4/30\n209/209 [==============================] - 91s 437ms/step - loss: 1.1176 - masked_acc: 0.7491 - val_loss: 1.8867 - val_masked_acc: 0.6105 - lr: 0.0010\nEpoch 5/30\n209/209 [==============================] - 90s 431ms/step - loss: 0.9561 - masked_acc: 0.7744 - val_loss: 1.8275 - val_masked_acc: 0.6185 - lr: 0.0010\nEpoch 6/30\n209/209 [==============================] - 90s 431ms/step - loss: 0.8486 - masked_acc: 0.7926 - val_loss: 1.7849 - val_masked_acc: 0.6265 - lr: 0.0010\nEpoch 7/30\n209/209 [==============================] - 90s 432ms/step - loss: 0.7667 - masked_acc: 0.8069 - val_loss: 1.7697 - val_masked_acc: 0.6295 - lr: 0.0010\nEpoch 8/30\n209/209 [==============================] - 90s 431ms/step - loss: 0.7017 - masked_acc: 0.8186 - val_loss: 1.7830 - val_masked_acc: 0.6338 - lr: 0.0010\nEpoch 9/30\n209/209 [==============================] - 90s 428ms/step - loss: 0.6436 - masked_acc: 0.8301 - val_loss: 1.7731 - val_masked_acc: 0.6362 - lr: 0.0010\nEpoch 10/30\n209/209 [==============================] - ETA: 0s - loss: 0.5961 - masked_acc: 0.8394\nEpoch 10: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\n209/209 [==============================] - 90s 430ms/step - loss: 0.5961 - masked_acc: 0.8394 - val_loss: 1.7848 - val_masked_acc: 0.6351 - lr: 0.0010\nEpoch 11/30\n209/209 [==============================] - 90s 430ms/step - loss: 0.4662 - masked_acc: 0.8720 - val_loss: 1.7535 - val_masked_acc: 0.6486 - lr: 1.0000e-04\nEpoch 12/30\n209/209 [==============================] - 90s 432ms/step - loss: 0.4356 - masked_acc: 0.8795 - val_loss: 1.7635 - val_masked_acc: 0.6485 - lr: 1.0000e-04\nEpoch 13/30\n209/209 [==============================] - 90s 429ms/step - loss: 0.4223 - masked_acc: 0.8825 - val_loss: 1.7816 - val_masked_acc: 0.6477 - lr: 1.0000e-04\nEpoch 14/30\n209/209 [==============================] - ETA: 0s - loss: 0.4126 - masked_acc: 0.8847\nEpoch 14: ReduceLROnPlateau reducing learning rate to 1.0000000656873453e-05.\n209/209 [==============================] - 90s 430ms/step - loss: 0.4126 - masked_acc: 0.8847 - val_loss: 1.7998 - val_masked_acc: 0.6465 - lr: 1.0000e-04\nEpoch 15/30\n209/209 [==============================] - 90s 428ms/step - loss: 0.3951 - masked_acc: 0.8900 - val_loss: 1.8006 - val_masked_acc: 0.6471 - lr: 1.0000e-05\nEpoch 16/30\n209/209 [==============================] - 90s 431ms/step - loss: 0.3935 - masked_acc: 0.8902 - val_loss: 1.8024 - val_masked_acc: 0.6476 - lr: 1.0000e-05\n","output_type":"stream"}],"execution_count":27},{"cell_type":"code","source":"model.save(\"BI-GRU-Cross-ATT.keras\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-19T16:56:00.741572Z","iopub.execute_input":"2025-12-19T16:56:00.741905Z","iopub.status.idle":"2025-12-19T16:56:01.484158Z","shell.execute_reply.started":"2025-12-19T16:56:00.741877Z","shell.execute_reply":"2025-12-19T16:56:01.483560Z"}},"outputs":[],"execution_count":28},{"cell_type":"code","source":"model.save_weights(\"BI-GRU-Cross-ATT.h5\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-19T16:56:01.485294Z","iopub.execute_input":"2025-12-19T16:56:01.485573Z","iopub.status.idle":"2025-12-19T16:56:01.605655Z","shell.execute_reply.started":"2025-12-19T16:56:01.485550Z","shell.execute_reply":"2025-12-19T16:56:01.604926Z"}},"outputs":[],"execution_count":29},{"cell_type":"code","source":"loaded_model = tf.keras.models.load_model(\"/kaggle/working/BI-GRU-Cross-ATT.keras\",compile=False)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-19T16:56:12.571160Z","iopub.execute_input":"2025-12-19T16:56:12.571748Z","iopub.status.idle":"2025-12-19T16:56:15.116310Z","shell.execute_reply.started":"2025-12-19T16:56:12.571695Z","shell.execute_reply":"2025-12-19T16:56:15.115701Z"}},"outputs":[],"execution_count":30},{"cell_type":"code","source":"def get_initial_state(model, context, target_text_processor):\n    if len(context.shape) == 1:\n        context = tf.expand_dims(context, 0)\n    \n    batch_size = tf.shape(context)[0]\n    vocab = target_text_processor.get_vocabulary()\n    start_token = vocab.index('[START]')\n    \n    next_token = tf.fill([batch_size, 1], start_token)\n    done = tf.zeros([batch_size, 1], dtype=tf.bool)\n    \n    return next_token, done, context\n\n\ndef get_next_token(model, context, next_token, done, state, target_text_processor, temperature=0.0):\n    vocab = target_text_processor.get_vocabulary()\n    end_token = vocab.index('[END]')\n    \n    padded_state = tf.pad(state, [[0, 0], [0, config['max_length'] - tf.shape(state)[1]]])[:, :config['max_length']]\n    logits = model.predict([context, padded_state], verbose=0)\n    last_logits = logits[:, tf.shape(state)[1] - 1, :]\n    \n    if temperature == 0.0:\n        next_token_id = tf.argmax(last_logits, axis=-1, output_type=tf.int32)\n    else:\n        next_token_id = tf.squeeze(tf.random.categorical(last_logits / temperature, 1, dtype=tf.int32), -1)\n    \n    next_token_id = tf.expand_dims(next_token_id, -1)\n    done = done | (next_token_id == end_token)\n    next_token_id = tf.where(done, tf.constant(0, dtype=tf.int32), next_token_id)\n    new_state = tf.concat([state, next_token_id], axis=1)\n    \n    return next_token_id, done, new_state\n\n\ndef translate(model, spanish_text, target_text_processor, temperature=0.0):\n    if len(spanish_text.shape) == 1:\n        spanish_text = tf.expand_dims(spanish_text, 0)\n    \n    next_token, done, context = get_initial_state(model, spanish_text, target_text_processor)\n    state = next_token\n    tokens = []\n    \n    for n in range(config['max_length']):\n        next_token, done, state = get_next_token(model, context, next_token, done, state, target_text_processor, temperature)\n        tokens.append(next_token)\n        if tf.reduce_all(done):\n            break\n    \n    tokens = tf.concat(tokens, axis=-1)\n    vocab = target_text_processor.get_vocabulary()\n    \n    words = []\n    for token_id in tokens[0].numpy():\n        if token_id == 0:\n            break\n        word = vocab[token_id]\n        if word == '[END]':\n            break\n        if word not in ['[START]', '[UNK]', '']:\n            words.append(word)\n    \n    return ' '.join(words)\n\n\ndef compare_translations(model, spanish_input, target_out, context_text_processor, target_text_processor, n=5):\n    spanish_vocab = context_text_processor.get_vocabulary()\n    english_vocab = target_text_processor.get_vocabulary()\n    \n    for i in range(min(n, spanish_input.shape[0])):\n        # Spanish input\n        sp_words = [spanish_vocab[t] for t in spanish_input[i].numpy() if t > 0]\n        spanish = ' '.join(sp_words)\n        \n        # Ground truth English\n        gt_words = [english_vocab[t] for t in target_out[i].numpy() \n                    if t > 0 and english_vocab[t] not in ['[START]', '[END]']]\n        ground_truth = ' '.join(gt_words)\n        \n        # Model translation\n        model_output = translate(model, spanish_input[i], target_text_processor)\n        \n        print(f\"\\n{i+1}. English: {spanish}\")\n        print(f\"   GROUNDTRUTH: {ground_truth}\")\n        print(f\"   TRANSLATION: {model_output}\")\n\n\n# Create distributed dataset + iterator\ndist_val_dataset = strategy.experimental_distribute_dataset(val_dataset)\ndist_val_iter = iter(dist_val_dataset)\n\n# ===== FIRST BATCH =====\n(ex_context_tok, ex_tar_in), ex_tar_out = next(dist_val_iter)\nex_context_tok = strategy.experimental_local_results(ex_context_tok)[0]\nex_tar_out     = strategy.experimental_local_results(ex_tar_out)[0]\n\ncompare_translations(\n    loaded_model,\n    ex_context_tok,\n    ex_tar_out,\n    context_text_processor,\n    target_text_processor,\n    n=5\n)\n\n# ===== SECOND BATCH =====\n(ex_context_tok, ex_tar_in), ex_tar_out = next(dist_val_iter)\nex_context_tok = strategy.experimental_local_results(ex_context_tok)[0]\nex_tar_out     = strategy.experimental_local_results(ex_tar_out)[0]\n\ncompare_translations(\n    loaded_model,\n    ex_context_tok,\n    ex_tar_out,\n    context_text_processor,\n    target_text_processor,\n    n=5\n)\n\n# ===== THIRD BATCH =====\n(ex_context_tok, ex_tar_in), ex_tar_out = next(dist_val_iter)\nex_context_tok = strategy.experimental_local_results(ex_context_tok)[0]\nex_tar_out     = strategy.experimental_local_results(ex_tar_out)[0]\n\ncompare_translations(\n    loaded_model,\n    ex_context_tok,\n    ex_tar_out,\n    context_text_processor,\n    target_text_processor,\n    n=5\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-19T16:56:15.117633Z","iopub.execute_input":"2025-12-19T16:56:15.118182Z","iopub.status.idle":"2025-12-19T16:56:29.979297Z","shell.execute_reply.started":"2025-12-19T16:56:15.118158Z","shell.execute_reply":"2025-12-19T16:56:29.978599Z"}},"outputs":[{"name":"stdout","text":"\n1. English: this is the same necklace that i lost yesterday .\n   GROUNDTRUTH: este collar es igual al que perdi ayer .\n   TRANSLATION: este es el mismo capaz de que perdi ayer .\n\n2. English: this is the strongest dog that i have ever seen .\n   GROUNDTRUTH: este es el perro mas fuerte que jamas haya visto .\n   TRANSLATION: este es el perro mas fuerte que haya visto jamas .\n\n3. English: this is your last chance to spend time with tom .\n   GROUNDTRUTH: esta es tu ultima oportunidad de pasar tiempo con tom .\n   TRANSLATION: esta es tu ultima oportunidad de tom con tom .\n\n4. English: this material will stand up to lots of [UNK] .\n   GROUNDTRUTH: este material [UNK] un monton de [UNK] .\n   TRANSLATION: este se a muchas .\n\n5. English: this medicine should be taken every three hours .\n   GROUNDTRUTH: este medicamento debe ser tomado cada tres horas .\n   TRANSLATION: este medicamento deberia haber tomado cada tres horas .\n\n1. English: his salary is double what it was seven years ago .\n   GROUNDTRUTH: su sueldo es el doble del de hace siete anos atras .\n   TRANSLATION: su salario es el doble de lo que era siete anos .\n\n2. English: how come you know so much about japanese history ?\n   GROUNDTRUTH: ¿ como es que sabes tanto sobre la historia de japon ?\n   TRANSLATION: ¿ como sabes tanto de la historia de japon ?\n\n3. English: how many christmas cards did you write last year ?\n   GROUNDTRUTH: ¿ cuantos [UNK] [UNK] el ano pasado ?\n   TRANSLATION: ¿ cuantas tarjetas de navidad escriba el ano pasado ?\n\n4. English: how many books do you think you have read so far ?\n   GROUNDTRUTH: ¿ cuantos libros crees que has leido hasta ahora ?\n   TRANSLATION: ¿ cuantos libros crees que has leido hasta ahora ?\n\n5. English: how many hours a day do you spend in your office ?\n   GROUNDTRUTH: ¿ cuantas horas al dia pasas en tu oficina ?\n   TRANSLATION: ¿ cuantas horas en dia te pasa en tu oficina ?\n\n1. English: the soldiers were ready to die for their country .\n   GROUNDTRUTH: los soldados estaban [UNK] a morir por su pais .\n   TRANSLATION: los soldados estaban listos para morir su pais .\n\n2. English: the storm prevented me from going out for a walk .\n   GROUNDTRUTH: la tormenta me impidio el salir a dar un paseo .\n   TRANSLATION: la tormenta me impidio salir a caminar .\n\n3. English: the [UNK] becomes [UNK] as you move [UNK] .\n   GROUNDTRUTH: la [UNK] es menos profundo a medida que [UNK] por el .\n   TRANSLATION: los se vuelve como .\n\n4. English: the strong wind [UNK] that a storm is coming .\n   GROUNDTRUTH: el fuerte viento [UNK] que se acerca una tormenta .\n   TRANSLATION: el viento fuerte que una tormenta se venga .\n\n5. English: the students are happy , but the teachers are not .\n   GROUNDTRUTH: los estudiantes son felices , pero los profesores no lo son .\n   TRANSLATION: los estudiantes estan felices , pero no los profesores .\n","output_type":"stream"}],"execution_count":31},{"cell_type":"code","source":"def translate_batch(model, spanish_batch, target_text_processor, temperature=0.0):\n    \"\"\"Translate a batch of sequences.\"\"\"\n    vocab = target_text_processor.get_vocabulary()\n    start_token = vocab.index('[START]')\n    end_token = vocab.index('[END]')\n    \n    batch_size = tf.shape(spanish_batch)[0]\n    state = tf.fill([batch_size, 1], start_token)\n    done = tf.zeros([batch_size], dtype=tf.bool)\n    \n    for _ in range(config['max_length']):\n        padded_state = tf.pad(state, [[0, 0], [0, config['max_length'] - tf.shape(state)[1]]])[:, :config['max_length']]\n        logits = model.predict([spanish_batch, padded_state], verbose=0)\n        last_logits = logits[:, tf.shape(state)[1] - 1, :]\n        \n        if temperature == 0.0:\n            next_token_id = tf.argmax(last_logits, axis=-1, output_type=tf.int32)\n        else:\n            next_token_id = tf.squeeze(tf.random.categorical(last_logits / temperature, 1, dtype=tf.int32), -1)\n        \n        next_token_id = tf.expand_dims(next_token_id, -1)\n        done = done | tf.squeeze(next_token_id == end_token, -1)\n        next_token_id = tf.where(tf.expand_dims(done, -1), 0, next_token_id)\n        state = tf.concat([state, next_token_id], axis=1)\n        \n        if tf.reduce_all(done):\n            break\n    \n    # Convert to sentences\n    results = []\n    for i in range(batch_size):\n        words = []\n        for token_id in state[i].numpy():\n            if token_id == 0 or token_id == end_token:\n                break\n            word = vocab[token_id]\n            if word not in ['[START]', '[END]', '[UNK]', '']:\n                words.append(word)\n        results.append(' '.join(words))\n    \n    return results","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-19T16:56:41.779764Z","iopub.execute_input":"2025-12-19T16:56:41.780466Z","iopub.status.idle":"2025-12-19T16:56:41.789168Z","shell.execute_reply.started":"2025-12-19T16:56:41.780437Z","shell.execute_reply":"2025-12-19T16:56:41.788548Z"}},"outputs":[],"execution_count":32},{"cell_type":"code","source":"refs = []\nhyps = []\nvocab = target_text_processor.get_vocabulary()\n\nfor (ex_context_tok, ex_tar_in), ex_tar_out in tqdm(val_dataset, desc=\"Calculating BLEU\"):\n    # Translate entire batch at once\n    preds = translate_batch(loaded_model, ex_context_tok, target_text_processor)\n    hyps.extend(preds)\n    \n    # Get references\n    for i in range(ex_context_tok.shape[0]):\n        ref_tokens = [vocab[t] for t in ex_tar_out[i].numpy() \n                     if t > 0 and vocab[t] not in ['[START]', '[END]']]\n        refs.append([' '.join(ref_tokens)])\n\n# Calculate BLEU\nbleu = sacrebleu.corpus_bleu(hyps, list(zip(*refs)))\nprint(f\"BLEU: {bleu.score:.2f}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-18T16:36:23.766941Z","iopub.execute_input":"2025-12-18T16:36:23.767577Z","iopub.status.idle":"2025-12-18T16:45:42.833701Z","shell.execute_reply.started":"2025-12-18T16:36:23.767552Z","shell.execute_reply":"2025-12-18T16:45:42.832997Z"}},"outputs":[{"name":"stderr","text":"Calculating BLEU: 100%|██████████| 23/23 [09:17<00:00, 24.26s/it]\nThat's 100 lines that end in a tokenized period ('.')\nIt looks like you forgot to detokenize your test data, which may hurt your score.\nIf you insist your data is detokenized, or don't care, you can suppress this message with the `force` parameter.\n","output_type":"stream"},{"name":"stdout","text":"BLEU: 21.92\n","output_type":"stream"}],"execution_count":45},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}